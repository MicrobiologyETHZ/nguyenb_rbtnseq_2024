{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compares different methods on running DESeq2 and linear models using a synthetic dataset\n",
    "\n",
    "# Conclusions: within taxa normalisation works almost as well as analysing each taxon individually, \n",
    "\n",
    "# Global normalisation is not appropriate \n",
    "\n",
    "# To do: look at the description of the synthetic dataset, re. how variable are different taxa between samples\n",
    "# To do: All the anlysis was done on filtered dataset (i.e. excluding low expression, redo without it?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import statsmodels\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import yaml\n",
    "with open(\"config.yaml\", \"r\") as fh:\n",
    "    config_dict = yaml.safe_load(fh)['default']\n",
    "from sklearn.metrics import precision_score, recall_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%ls /nfs/nas22/fs2202/biol_micro_bioinf_nccr/hardt/nguyenb/tnseq/scratch/03_23_transcriptomics/modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = Path(config_dict[\"root\"])\n",
    "synth_dir = root/config_dict[\"synth_dir\"]\n",
    "sample_data_file = root/config_dict[\"sample_data_file\"]\n",
    "spikes = pd.read_table(synth_dir/\"synth_mgx_mtx/true-exp.mtx_spiked.tsv\", header=None, names=['feature', 'positive'])\n",
    "spikes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spikes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving synthetic data in the format accepted by deseq2. \n",
    "- Added 1 because with such low 'depth' DESeq2 fails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "dsdf = pd.read_table(synth_dir/\"synth_mgx_mtx/true-exp.mtx_abunds.tsv\", index_col=0).T.reset_index().rename(columns={'index': 'sample_id'})\n",
    "ds_meta = dsdf[['sample_id', 'Phenotype']]\n",
    "dsdf = dsdf.drop(columns=['Phenotype', 'SeqDepth']).set_index('sample_id').T.reset_index().rename(columns={'#':'ID'}).set_index('ID')\n",
    "dsdf = dsdf + 1\n",
    "#ds_meta.to_csv(synth_dir/'true-exp-meta.csv', index=False)\n",
    "#dsdf.reset_index().to_csv(synth_dir/\"true-exp-counts.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DESeq results on all samples with 1 added to all counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Analysis with global normalisation\n",
    "deres = pd.read_csv(synth_dir/\"2023-07-31_true-exp-deseq-1_vs_0_l0a0.01_results.csv\")\n",
    "# Analysis with taxon specific normalisation\n",
    "deres_tx = pd.read_csv(synth_dir/\"2023-07-31_true-exp-deseq-within-taxon-1_vs_0_l0a0.01_results.csv\")\n",
    "# Analysis of taxa one by one\n",
    "deres_1tx = pd.read_csv(synth_dir/\"2023-08-03_true-exp-deseq-taxon-one-by-one-1_vs_0_l0a0.01_results.csv\", index_col=0 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores(result_df, ground_truth, left_feat, right_feat, name, fdr=0.05):\n",
    "    df = (result_df[[left_feat, 'padj']]\n",
    "          .merge(ground_truth, left_on=left_feat, right_on=right_feat, how='outer')\n",
    "          .dropna(subset=[left_feat]))\n",
    "    predicted_phenotype_tx = (df.padj < fdr).astype(int)\n",
    "    actual_phenotype_tx = (df.positive.notnull()).astype(int)\n",
    "    return  pd.Series([precision_score(actual_phenotype_tx, predicted_phenotype_tx), \n",
    "                       recall_score(actual_phenotype_tx, predicted_phenotype_tx)] + list(confusion_matrix(actual_phenotype_tx, predicted_phenotype_tx).ravel().astype(int)),index=['Precision', 'Recall', 'TN', 'FP', 'FN', 'TP'], name=name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdr = 0.01\n",
    "score_list = []\n",
    "for res, label in zip([deres, deres_tx, deres_1tx], ['Global', 'Taxon', 'IndivTaxon']):\n",
    "    score_list.append(get_scores(res, spikes, 'ID', 'feature', label, fdr))\n",
    "\n",
    "scores = pd.DataFrame(score_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frp = scores['FP']/(scores['FP'] + scores['TN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Same analysis with linear models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_table(synth_dir/\"synth_mgx_mtx/true-exp.mtx_abunds.tsv\", index_col=0).T.reset_index().rename(columns={'index': 'sample_id'})\n",
    "df = df.melt(id_vars=[\"sample_id\", \"Phenotype\", \"SeqDepth\"], var_name=\"gene_name\", value_name='raw_cnt')\n",
    "df['tss'] = df['raw_cnt']/df['SeqDepth']*1e6\n",
    "df['bug'] = df['gene_name'].str.split(\"_\", expand=True)[0]\n",
    "df = df.merge(df.groupby(['sample_id', 'bug']).raw_cnt.sum().reset_index().rename(columns={'raw_cnt':'bug_cnt'}), on=['sample_id', 'bug'], how='left')\n",
    "df['within_bug'] = df['raw_cnt']/df['bug_cnt']*1e6\n",
    "df['bug_perc'] = df['bug_cnt']/df['SeqDepth']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out genes with less than 100 reads across samples\n",
    "mask = df.groupby(['gene_name']).raw_cnt.sum() > 100\n",
    "mask2 = df[df.raw_cnt > 0].groupby('gene_name').sample_id.nunique() > 10\n",
    "df_filtered = df.set_index('gene_name').loc[mask&mask2,:].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6818 true positives left in the dataset\n",
    "df_filtered.merge(spikes, left_on='gene_name', right_on='feature', how='inner')[['gene_name', 'positive']].drop_duplicates().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_on_gene(gene_df, expr_col, fixed_effects):\n",
    "    pseudo_count = gene_df[gene_df[expr_col] > 0][expr_col].min()/2\n",
    "    gene_df[f'{expr_col}_log'] = np.log10(gene_df[expr_col] + pseudo_count)\n",
    "    #gene_df['tss_log_std'] = (gene_df['tss_log'] - gene_df['tss_log'].mean())/gene_df['tss_log'].std()\n",
    "    if not gene_df.empty and any([i in gene_df.columns for i in fixed_effects]):\n",
    "        formula = f\"{expr_col}_log ~ {' + '.join(fixed_effects)}\"\n",
    "        #print(formula)\n",
    "        md = smf.glm(formula=formula, data=gene_df, family=sm.families.Gaussian()).fit()\n",
    "        #print(md.summary())\n",
    "        return [md.params[1], md.bse[1], md.pvalues[1]]\n",
    "    return []\n",
    "\n",
    "def linear_on_df(df, expr_col, fixed_effects):\n",
    "    f = df.dropna().groupby('gene_name').apply(linear_on_gene, expr_col = expr_col, fixed_effects = fixed_effects).reset_index()\n",
    "    f[['coef', 'se', 'pval']] = pd.DataFrame(f[0].to_list(), index = f.index)\n",
    "    f = f.drop(columns=[0])\n",
    "    f['padj'] = statsmodels.stats.multitest.multipletests(f.pval.values, method='fdr_bh')[1]\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phenotype_tx = linear_on_df(df_filtered, 'within_bug', ['Phenotype'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phenotype_bug_tx = linear_on_df(df_filtered, 'within_bug', ['Phenotype', 'bug_perc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phenotype_global = linear_on_df(df_filtered, 'tss', ['Phenotype'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdr = 0.05\n",
    "score_list = []\n",
    "for res, label in zip([phenotype_tx, phenotype_bug_tx, phenotype_global], ['LM_taxon_Ph', 'LM_taxon_Ph_bug', 'LM_global']):\n",
    "    score_list.append(get_scores(res, spikes, 'gene_name', 'feature', label, fdr))\n",
    "scores_lm = pd.DataFrame(score_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([scores, scores_lm])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datavis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
