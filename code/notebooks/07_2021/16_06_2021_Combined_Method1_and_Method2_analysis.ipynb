{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extreme-bidder",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from scipy.stats import ranksums\n",
    "import matplotlib.pyplot as plt\n",
    "import chart_studio\n",
    "import chart_studio.tools as tls\n",
    "import chart_studio.plotly as py\n",
    "import plotly.express as px\n",
    "%matplotlib inline\n",
    "import cufflinks as cf\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "init_notebook_mode(connected=True)\n",
    "cf.go_offline()\n",
    "import dash_bio as dashbio\n",
    "import os\n",
    "\n",
    "from quality_control import *\n",
    "from method1_analysis import *\n",
    "from method2_analysis import *\n",
    "from final_analysis import * "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "declared-index",
   "metadata": {},
   "source": [
    "# Table of Contents: <a id='start'></a>\n",
    "\n",
    "1. [Loading the data](#loading-data)\n",
    "2. [Filtering the data](#filtering)\n",
    "2. [Method 1](#Method-1)\n",
    "3. [Method 2](#Method-2)\n",
    "4. [Compare the results](#Compare)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incoming-arena",
   "metadata": {},
   "source": [
    "# Loading the data <a id='loading-data'></a>\n",
    "\n",
    "\n",
    "## Data Analysed:\n",
    "\n",
    "List of dnaids\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "auburn-plane",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Todo: move file directories to config files, so that can be re-run with different counts\n",
    "\n",
    "counts_dir =\"/Users/ansintsova/git_repos/nguyenb_tnseq/data/13_04_results/counts\"\n",
    "outdir = '/Users/ansintsova/git_repos/nguyenb_tnseq/data/07_06_results/'\n",
    "control_file = Path(\"/Users/ansintsova/git_repos/nguyenb_tnseq/data/13_04_results\")/'controls.txt'\n",
    "\n",
    "# Load\n",
    "dnaids = ['dnaid1315', 'dnaid1428', 'dnaid1429', 'dnaid1457', 'dnaid2015', 'dnaid2016', 'dnaid2017', 'dnaid2018', 'dnaid2019',\n",
    "         'dnaid2023', 'dnaid2024', 'dnaid2025', 'dnaid2026', 'dnaid2027', 'dnaid2028', 'dnaid2029' ]\n",
    "\n",
    "cnt_df = load_files(dnaids, Path(counts_dir))\n",
    "# Create unique identifier for each sample\n",
    "cnt_df['sampleID'] = cnt_df['sampleID'] + \"_\" + cnt_df['dnaid'] + \"_\" + cnt_df['experiment']\n",
    "cnt_df = cnt_df[cnt_df.sampleID.notnull()]\n",
    "cnt_df['CntrlName'] = cnt_df['phenotype'] + cnt_df['conc'].astype(str)\n",
    "cnt_df['ShortName'] = cnt_df.ShortName.fillna(cnt_df.CntrlName)\n",
    "\n",
    "# Dropping Unenriched samples\n",
    "cnt_df = cnt_df[~cnt_df.sampleID.str.contains('unenriched')]\n",
    "annotation_df = cnt_df[['barcode', 'ShortName', 'locus_tag', 'phenotype', 'conc']].drop_duplicates()\n",
    "\n",
    "libraries = [lib for lib in cnt_df.library.unique() if type(lib) == str]\n",
    "libraries.remove('library_14_1')\n",
    "print(len(libraries))\n",
    "days = ['_d1', '_d2', '_d3', '_d4']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recent-yesterday",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDir = Path(\"/Users/ansintsova/git_repos/nguyenb_tnseq/data/08_21\")\n",
    "cnt_df.to_csv(dataDir/'old_counts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acoustic-daily",
   "metadata": {},
   "outputs": [],
   "source": [
    "libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "professional-teaching",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt_df.groupby('library').experiment.nunique().reset_index().sort_values('library')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "julian-disaster",
   "metadata": {},
   "outputs": [],
   "source": [
    "mice_per_library = cnt_df.groupby(['library', 'day']).sampleID.nunique().reset_index()\n",
    "mice_per_library\n",
    "# for library in mice_per_library.library.unique():\n",
    "#     print (mice_per_library[mice_per_library.library == library])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "competitive-franklin",
   "metadata": {},
   "source": [
    "## Summary:\n",
    "\n",
    "From the above results, not enough data for library_14_1, should be dropped. At least attach warning to results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "norwegian-thinking",
   "metadata": {},
   "source": [
    "# Filtering the data: <a id='filtering'></a>\n",
    "\n",
    "## 1. [Checking for linearity of control dilutions](#check-linearity)\n",
    "\n",
    "## 2. [Filtering out samples with skewed WT fitness ](#check-wt-fitness)\n",
    "\n",
    "\n",
    "[Back to the start](#start)\n",
    "\n",
    "[Next to Method 1](#Method-1)\n",
    "\n",
    "[Next to Method 2](#Method-2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "injured-wednesday",
   "metadata": {},
   "source": [
    "# Checking for linearity of control dilutions <a id='check-linearity'></a>\n",
    "\n",
    "\n",
    "- Cutting off samples with $R^2$ < 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "behavioral-infection",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "three-september",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_df, good_samples = calculate_correlation(cnt_df, control_file, for_each='sampleID')\n",
    "print(len(good_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "juvenile-metadata",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt_df.mouse.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "irish-monte",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set([c.split(\"_\")[0] for c in good_samples]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interstate-virgin",
   "metadata": {},
   "outputs": [],
   "source": [
    "def viz_linearity(cnt_df, phenotype='wt', day='d0', library = 'library_10_2'):\n",
    "    query = f\"(phenotype == '{phenotype}') & (day == '{day}') & (library == '{library}')\"\n",
    "    df = cnt_df.query(query)[['barcode', 'sampleID', 'cnt', 'conc', 'library']]\n",
    "    df['lconc'] = np.log(df.conc)\n",
    "    df['lcnt'] = np.log(df.cnt +1)\n",
    "    fig = px.scatter(df.sort_values('sampleID'), \n",
    "                     x=\"lconc\", y=\"lcnt\", facet_col=\"sampleID\", facet_col_wrap=3,height=3000, width=800,\n",
    "                     trendline='ols')\n",
    "    return df, fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alleged-uganda",
   "metadata": {},
   "outputs": [],
   "source": [
    "wdf, fig= viz_linearity(cnt_df, day='d2', library='library_10_2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "filled-limitation",
   "metadata": {},
   "source": [
    "# Filtering out samples with skewed WT fitness <a id='check-wt-fitness'></a>\n",
    "\n",
    "- Median $log_2FC$ within -1/1 for WT barcodes\n",
    "- Filtering out technical artifacts of unknown origin\n",
    "\n",
    "### How to:\n",
    "- Only used 'good samples' identified above\n",
    "- Calculate barcode fitness for each lbirary.\n",
    "    - For each experiment in each library:\n",
    "        - Perform VST transformation\n",
    "        - Calculate fitness for each barcode $\\frac{2^{vst-barcode-counts}}{2^{vst-inoculum-counts}}$\n",
    "- Calculate WT barcode fitnesses \n",
    "- Identify samples with abs($log_2FC$) > 1\n",
    "\n",
    "[Back to the start](#start)\n",
    "\n",
    "[Next to Method 1](#Method-1)\n",
    "\n",
    "[Next to Method 2](#Method-2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heated-shanghai",
   "metadata": {},
   "outputs": [],
   "source": [
    "skewed_samples = []\n",
    "no_inoc_samples = []\n",
    "all_wt_fit = []\n",
    "for library in libraries:\n",
    "    print(library)\n",
    "    library_vst, library_barcode_fitness = get_barcode_fitness_by_library(cnt_df, library, good_samples, outdir, filter_below=0)\n",
    "    library_wt_fitness = get_wt_fitness_by_library(library_barcode_fitness, annotation_df, phenotype='wt')\n",
    "    all_wt_fit.append(library_wt_fitness.assign(library=library))\n",
    "    no_inoc_samples += list(library_wt_fitness.isna().all()[lambda x: x].index)\n",
    "    skewed_samples += list(get_skewed(library_wt_fitness))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "diverse-parcel",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_inoc_samples = ['w435_d3_dnaid1457_TV3522A',\n",
    " 'w436_d3_dnaid1457_TV3522A',\n",
    " 'w441_d4_dnaid1457_TV3522C',\n",
    " 'w443_d4_dnaid1457_TV3522C',\n",
    " 'w445_d4_dnaid1457_TV3522D',\n",
    " 'w446_d3_dnaid1457_TV3522D',\n",
    " 'w446_d4_dnaid1457_TV3522D']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "announced-california",
   "metadata": {},
   "outputs": [],
   "source": [
    "skewed_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dangerous-directory",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(Path(outdir)/'skewed_samples.txt', 'w') as fh:\n",
    "    for s in skewed_samples:\n",
    "        fh.write(f\"{s}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legitimate-lobby",
   "metadata": {},
   "source": [
    "# Method 1: <a id='Method-1'></a>\n",
    "\n",
    "\n",
    "- Take all the library samples, run DESeq to get fitness values\n",
    "\n",
    "\n",
    "[Back to the start](#start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "discrete-singles",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(Path(outdir)/\"skewed_samples.txt\", 'r') as fh:\n",
    "    skewed_samples = [s.strip() for s in fh.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accepted-commerce",
   "metadata": {},
   "outputs": [],
   "source": [
    "skewed_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hairy-evening",
   "metadata": {},
   "source": [
    "## Method 1\n",
    "\n",
    "- Get counts for each gene as sum of all transposons mapped to that gene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "limiting-christmas",
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_df = cnt_df.groupby(['sampleID', 'ShortName', 'mouse', 'day', 'library', 'tissue', 'dnaid', 'experiment' ]).cnt.sum().reset_index().rename({'ShortName':'barcode'}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "catholic-guitar",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "vsts = {}\n",
    "for library in libraries:\n",
    "    print(library)\n",
    "    exp_df =  gene_df[gene_df.library == library].copy()\n",
    "    library_samples = [s for s in good_samples if s in exp_df.sampleID.unique() and s not in skewed_samples  and s not in no_inoc_samples]\n",
    "    fit, res, vst = analyze_library2(exp_df, sample_id=\"sampleID\", \n",
    "                              good_samples=library_samples, \n",
    "                              dnaid=library.replace(\"_\", \"-\"), experiment='', \n",
    "                              control_file=control_file, \n",
    "                              to_filter=1000, outdir=outdir)\n",
    "    fit.columns = ['gene', 'baseMean', 'log2FC', 'lfcSE', 'stat', 'lfc_pvalue', 'lfc_padj', 'day', 'n_samples']\n",
    "    res.columns = ['gene', 'day', 'gene_FC', 'sigma', 'z-score', 'CI', 'zscore_pval', 'zscore_padj']\n",
    "    final = fit[['gene', 'log2FC', 'lfcSE', 'lfc_pvalue', 'lfc_padj', 'day', 'n_samples']].merge(res[['gene', 'day', 'z-score', 'CI', 'zscore_pval', 'zscore_padj']], how='outer', on=['gene', 'day'])\n",
    "    final = final[['gene', 'day', 'log2FC', 'lfcSE', 'lfc_pvalue', 'lfc_padj', 'z-score', 'CI', 'zscore_pval', 'zscore_padj']].assign(library=library)\n",
    "    results.append(final)\n",
    "    vsts[library] = vst\n",
    "    \n",
    "final_m1_1000 = pd.concat(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "published-savage",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "\n",
    "a_file = open(Path(outdir)/\"27-07-vsts.pkl\", \"wb\")\n",
    "pickle.dump(vsts, a_file)\n",
    "a_file.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifty-slovenia",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# results = []\n",
    "# for library in libraries:\n",
    "#     print(library)\n",
    "#     exp_df =  gene_df[gene_df.library == library].copy()\n",
    "#     library_samples = [s for s in good_samples if s in exp_df.sampleID.unique() and s not in skewed_samples  and s not in no_inoc_samples]\n",
    "#     fit, res = analyze_library2(exp_df, sample_id=\"sampleID\", \n",
    "#                               good_samples=library_samples, \n",
    "#                               dnaid=library.replace(\"_\", \"-\"), experiment='', \n",
    "#                               control_file=control_file, \n",
    "# #                              to_filter=100, outdir=outdir)\n",
    "#     fit.columns = ['gene', 'baseMean', 'log2FC', 'lfcSE', 'stat', 'lfc_pvalue', 'lfc_padj', 'day', 'n_samples']\n",
    "#     res.columns = ['gene', 'day', 'gene_FC', 'sigma', 'z-score', 'CI', 'zscore_pval', 'zscore_padj']\n",
    "#     final = fit[['gene', 'log2FC', 'lfcSE', 'lfc_pvalue', 'lfc_padj', 'day', 'n_samples']].merge(res[['gene', 'day', 'z-score', 'CI', 'zscore_pval', 'zscore_padj']], how='outer', on=['gene', 'day'])\n",
    "#     final = final[['gene', 'day', 'log2FC', 'lfcSE', 'lfc_pvalue', 'lfc_padj', 'z-score', 'CI', 'zscore_pval', 'zscore_padj']].assign(library=library)\n",
    "#     results.append(final)\n",
    "# final_m1_100 = pd.concat(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorporate-rouge",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_m1_1000.to_csv(Path(outdir)/'27-07-results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boxed-bridal",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_m1_1000[final_m1_1000.gene.str.len() < 10].gene.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quick-pressure",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_m1_1000[(final_m1_1000.gene.str.len() < 10) & (final_m1_1000.zscore_padj<0.05)].groupby('day').gene.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collect-conservation",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_m1_1000[(final_m1_1000.gene.str.len() < 10) & (final_m1_1000.zscore_padj<0.05)].groupby(['library','day']).gene.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modular-volleyball",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_m1_100[final_m1_100.gene.str.len() < 10].gene.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sensitive-celebration",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_m1_100[(final_m1_100.gene.str.len() < 10) & (final_m1_100.zscore_padj<0.05)].groupby('day').gene.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "announced-croatia",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "standing-shoot",
   "metadata": {},
   "outputs": [],
   "source": [
    "approx_pos = cnt_df[['ShortName','library',  'sstart', 'sseqid']].drop_duplicates().groupby(['ShortName',  'library', 'sseqid']).sstart.min().reset_index().rename({'ShortName':'gene'}, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "labeled-equipment",
   "metadata": {},
   "outputs": [],
   "source": [
    "approx_pos.to_csv(Path(outdir)/\"approx_pos.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accredited-commons",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt_df[['ShortName','library',  'sstart', 'sseqid']][cnt_df.ShortName == 'siiE'].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interesting-manual",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_m1_1000_pos = final_m1_1000.merge(approx_pos, on = ['gene', 'library'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "judicial-official",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_m1_1000_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unavailable-architect",
   "metadata": {},
   "source": [
    "## Remove skewed and samples with no inoculum, filter 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "competitive-richmond",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = final_m1_1000_pos[(final_m1_1000_pos.day == 'd2')&(final_m1_1000_pos.sseqid == 'FQ312003.1')]\n",
    "test2 = test[test.zscore_padj < 0.05]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "desperate-exercise",
   "metadata": {},
   "outputs": [],
   "source": [
    "test2[(test2.sstart > 1300000) & (test2.sstart<1500000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mental-interest",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = final_m1_1000_pos[(final_m1_1000_pos.day == 'd1')&(final_m1_1000_pos.sseqid == 'FQ312003.1')]\n",
    "test2 = test[(test.zscore_padj < 0.05) & (abs(np.log2(test.CI)) > 1)]\n",
    "\n",
    "\n",
    "plt.figure(figsize = (40, 10))\n",
    "plt.scatter(test.sstart, test.CI, color='grey', alpha=0.2)\n",
    "plt.scatter(test2.sstart, test2.CI, color='red', alpha=0.5)\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cognitive-attitude",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = final_m1_1000_pos[(final_m1_1000_pos.day == 'd2')&(final_m1_1000_pos.sseqid == 'FQ312003.1')]\n",
    "test2 = test[(test.zscore_padj < 0.05) & (abs(np.log2(test.CI)) > 1)]\n",
    "\n",
    "plt.figure(figsize = (40, 10))\n",
    "plt.scatter(test.sstart, test.CI, color='grey', alpha=0.2)\n",
    "plt.scatter(test2.sstart, test2.CI, color='red', alpha=0.5)\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "functioning-party",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_m1_1000_pos.sseqid.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fancy-audio",
   "metadata": {},
   "outputs": [],
   "source": [
    "test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deluxe-architecture",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "test = final_m1_1000_pos[(final_m1_1000_pos.sseqid == 'FQ312003.1')]\n",
    "test2 = test[(test.zscore_padj < 0.05) & (abs(np.log2(test.CI)) > 1)].sort_values('day')\n",
    "plt.figure(figsize = (40, 10))\n",
    "sns.stripplot(x = test2.sstart, y=test2.day)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "varying-stations",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final2 = test[['CI', 'day', 'gene', 'library']].pivot(index=['gene', 'library'], columns='day', values='CI').reset_index().set_index('gene')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "focal-singapore",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final2.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "responsible-evolution",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = final_m1_1000_pos[(final_m1_1000_pos.day == 'd3')&(final_m1_1000_pos.sseqid == 'FQ312003.1')]\n",
    "test2 = test[(test.zscore_padj < 0.05) & (abs(np.log2(test.CI)) > 1)]\n",
    "\n",
    "plt.figure(figsize = (40, 10))\n",
    "plt.scatter(test.sstart, test.CI, color='grey',alpha=0.2)\n",
    "plt.scatter(test2.sstart, test2.CI, color='red', alpha=0.5)\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "considerable-walker",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = final_m1_1000_pos[(final_m1_1000_pos.day == 'd4')&(final_m1_1000_pos.sseqid == 'FQ312003.1')]\n",
    "test2 = test[(test.zscore_padj < 0.05) & (abs(np.log2(test.CI)) > 1)]\n",
    "\n",
    "\n",
    "plt.figure(figsize = (40, 10))\n",
    "plt.scatter(test.sstart, test.CI, color='grey', alpha=0.2)\n",
    "plt.scatter(test2.sstart, test2.CI, color='red', alpha=0.5)\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "original-posting",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for library in ['library_10_2']:\n",
    "    print(library)\n",
    "    lib_df = cnt_df[cnt_df.library == library].copy()\n",
    "    #Removing some noise\n",
    "    lib_df = lib_df[~((lib_df.libcnt.isna()) & (lib_df.phenotype.isna()))]\n",
    "    library_samples = [s for s in good_samples if s in lib_df.sampleID.unique() and s not in skewed_samples \n",
    "                      and s not in no_inoc_samples]\n",
    "    fit, res = analyze_library(lib_df, sample_id=\"sampleID\", \n",
    "                          good_samples=library_samples, \n",
    "                          dnaid=library.replace(\"_\", \"-\"), experiment='', \n",
    "                          control_file=control_file, \n",
    "                          to_filter=100, outdir=outdir)\n",
    "    results.append(res)\n",
    "    \n",
    "method1_skewed_removed = pd.concat(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "relative-extension",
   "metadata": {},
   "outputs": [],
   "source": [
    "m1_comp = method1_skewed_removed.merge(m1_b_res, on=['gene', 'day'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plastic-fault",
   "metadata": {},
   "outputs": [],
   "source": [
    "m1_comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boring-selling",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(m1_comp.ci_x, m1_comp.ci_y)\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "substantial-performer",
   "metadata": {},
   "source": [
    "## Remove skewed and samples with no inoculum, filter 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "single-maryland",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for library in libraries:\n",
    "    print(library)\n",
    "    lib_df = cnt_df[cnt_df.library == library].copy()\n",
    "    #Removing some noise\n",
    "    lib_df = lib_df[~((lib_df.libcnt.isna()) & (lib_df.phenotype.isna()))]\n",
    "    library_samples = [s for s in good_samples if s in lib_df.sampleID.unique() and s not in skewed_samples \n",
    "                      and s not in no_inoc_samples]\n",
    "    fit, res = analyze_library(lib_df, sample_id=\"sampleID\", \n",
    "                          good_samples=library_samples, \n",
    "                          dnaid=library.replace(\"_\", \"-\"), experiment='', \n",
    "                          control_file=control_file, \n",
    "                          to_filter=100, outdir=outdir)\n",
    "    results.append(res)\n",
    "    \n",
    "method1_skewed_removed_100 = pd.concat(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hungry-category",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for library in libraries:\n",
    "    print(library)\n",
    "    lib_df = cnt_df[cnt_df.library == library].copy()\n",
    "    #Removing some noise\n",
    "    lib_df = lib_df[~((lib_df.libcnt.isna()) & (lib_df.phenotype.isna()))]\n",
    "    library_samples = [s for s in good_samples if s in lib_df.sampleID.unique() if s not in no_inoc_samples]\n",
    "    fit, res = analyze_library(lib_df, sample_id=\"sampleID\", \n",
    "                          good_samples=library_samples, \n",
    "                          dnaid=library.replace(\"_\", \"-\"), experiment='', \n",
    "                          control_file=control_file, \n",
    "                          to_filter=1000, outdir=outdir)\n",
    "    results.append(res)\n",
    "    \n",
    "method1_with_skewed = pd.concat(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spatial-blair",
   "metadata": {},
   "outputs": [],
   "source": [
    "method1_skewed_removed_100.to_csv(Path(outdir)/\"21-07-method1_skewed_removed_100.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "macro-groove",
   "metadata": {},
   "outputs": [],
   "source": [
    "method1_skewed_removed.to_csv(Path(outdir)/\"21-07-method1_skewed_removed.csv\")\n",
    "method1_with_skewed.to_csv(Path(outdir)/\"21-07-method1_with_skewed.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "engaging-python",
   "metadata": {},
   "source": [
    "# Method 2: <a id='Method-2'></a>\n",
    "\n",
    "[Back to the start](#start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "received-japanese",
   "metadata": {},
   "outputs": [],
   "source": [
    "good_samples_noskew = [s for s in good_samples if s not in skewed_samples and s not in no_inoc_samples]\n",
    "vsts = {}\n",
    "fits = []\n",
    "cis = []\n",
    "wt_fits = []\n",
    "ssa_ci = []\n",
    "resultsFitList = []\n",
    "resultsCIList=[]\n",
    "# Fitness Results\n",
    "for library in libraries:\n",
    "    print(library)\n",
    "    library_vst, library_barcode_fitness = get_barcode_fitness_by_library(cnt_df, library, good_samples_noskew, outdir, filter_below=1000)\n",
    "    library_gene_fitness = get_gene_fitness_by_library(library_barcode_fitness, annotation_df)\n",
    "    library_wt_fitness = get_wt_fitness_by_library(library_barcode_fitness, annotation_df, phenotype='wt')\n",
    "    library_ssa_fitness = get_wt_fitness_by_library(library_barcode_fitness, annotation_df, phenotype='ssaV_invG')\n",
    "    library_gene_ci = library_gene_fitness.set_index('ShortName').apply(lambda x: x / library_wt_fitness.median()[x.name]).reset_index()\n",
    "    library_ssa_ci = library_ssa_fitness.median()/library_wt_fitness.median()\n",
    "    meltGeneFit = melt_sampleID(library_gene_fitness, idVar=['ShortName'], value_name='fitness')\n",
    "    meltWtFit = melt_sampleID(library_wt_fitness, idVar=['barcode', 'phenotype', 'conc'], value_name='fitness')\n",
    "    meltGeneCI = melt_sampleID(library_gene_ci, idVar=['ShortName'], value_name='ci')\n",
    "    resultsFit = get_library_results(meltGeneFit, meltWtFit, library)\n",
    "    resultsCI = get_library_results_ci(meltGeneCI, library_ssa_ci, library)\n",
    "    \n",
    "    fits.append(meltGeneFit.assign(library=library))\n",
    "    cis.append(meltGeneCI.assign(library=library))\n",
    "    wt_fits.append(meltWtFit.assign(library=library))\n",
    "    resultsFitList.append(resultsFit)\n",
    "    resultsCIList.append(resultsCI)\n",
    "    ssa_ci.append(pd.DataFrame(library_ssa_ci, columns=['ssa_ci']).assign(library=library))\n",
    "    vsts[library]= library_vst\n",
    "      \n",
    "m2_fits_no_skew = pd.concat(fits)\n",
    "m2_ci_no_skew = pd.concat(cis)\n",
    "m2_wt_fits_no_skew = pd.concat(wt_fits)\n",
    "m2_results_fit_no_skew = pd.concat(resultsFitList)\n",
    "m2_results_ci_no_skew = pd.concat(resultsCIList)\n",
    "m2_ssa_ci_no_skew = pd.concat(ssa_ci)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rocky-duration",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_map = {s:['median'] for s in library_vst}\n",
    "sample_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "falling-lounge",
   "metadata": {},
   "outputs": [],
   "source": [
    "genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "going-graduate",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brutal-worker",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_map = {s:['median'] for s in vsts['library_14_2']}\n",
    "test_gene = (vsts['library_14_2'].reset_index()\n",
    " .merge(annotation_df, on='barcode').drop(['locus_tag', 'phenotype', 'conc'], axis=1)\n",
    " .set_index('barcode')\n",
    " .groupby('ShortName').agg(sample_map))\n",
    "test_gene.columns = [c[0] for c in test_gene.columns]\n",
    "genes = test_gene.var(axis=1).sort_values(ascending=False).head(50).index\n",
    "test_gene = test_gene.loc[genes]\n",
    "df, pc1, pc2 = get_pca_df(test_gene)\n",
    "plotPCA(df, pc1,pc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "welsh-stranger",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def get_pca_df(library_vst, num_genes=500):\n",
    "    var_bcs = library_vst.var(axis=1).sort_values(ascending=False).head(num_genes).index\n",
    "    df = library_vst.loc[var_bcs]\n",
    "    meta = library_vst.T.reset_index().rename({'index':'sampleID'}, axis=1)\n",
    "    new = meta.sampleID.str.split(\"_\", expand=True)\n",
    "    new.columns = ['mouse', 'day', 'dnaid', 'experiment']\n",
    "    meta = pd.concat([meta[['sampleID']], new], axis=1).set_index('sampleID')\n",
    "    pDf, pc1_var, pc2_var = find_pc1_pc2(df, meta)\n",
    "    return pDf, pc1_var, pc2_var\n",
    "\n",
    "\n",
    "def plotPCA(pDf, pc1, pc2, title=\"\"):\n",
    "    fig = px.scatter(pDf.sort_values('day'), x='PC1', y='PC2', color='day', symbol='experiment', hover_data=['mouse'],\n",
    "              template='simple_white', title=title,\n",
    "              color_discrete_sequence=px.colors.qualitative.G10,\n",
    "                labels ={'PC1': f'PC1, {pc1}%',\n",
    "                        'PC2': f'PC2, {pc2}%',\n",
    "                        'day': 'Day',\n",
    "                        'experiment': 'Experiment'})\n",
    "\n",
    "\n",
    "    fig.update_traces(marker=dict(size=12,\n",
    "                                  line=dict(width=2,\n",
    "                                            color='DarkSlateGrey')),\n",
    "                      selector=dict(mode='markers'))\n",
    "\n",
    "    fig.update_layout(\n",
    "        font_family=\"Arial\",\n",
    "        font_size=14,\n",
    "        title_font_size=24,\n",
    "        title_x=0.5\n",
    "    )\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clean-arthur",
   "metadata": {},
   "outputs": [],
   "source": [
    "pDf, pc1, pc2 = get_pca_df(vsts['library_10_2'], 50)\n",
    "plotPCA(pDf, pc1, pc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civil-jenny",
   "metadata": {},
   "outputs": [],
   "source": [
    "figs = []\n",
    "\n",
    "for library in libraries:\n",
    "    pDf, pc1, pc2 = get_pca_df(vsts[library])\n",
    "    figs.append(plotPCA(pDf, pc1, pc2, library))\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decreased-corrections",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(Path(outdir)/'PCA_barcode_abundance.html', 'a') as f:\n",
    "    for fig in figs:\n",
    "        f.write(fig.to_html())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cubic-luxury",
   "metadata": {},
   "outputs": [],
   "source": [
    "figs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sought-yorkshire",
   "metadata": {},
   "outputs": [],
   "source": [
    "figs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heated-germany",
   "metadata": {},
   "outputs": [],
   "source": [
    "figs[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opened-fault",
   "metadata": {},
   "outputs": [],
   "source": [
    "figs[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attractive-rover",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gene.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rational-purchase",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "fig = plt.figure(figsize=(20,20))\n",
    "ax = sns.clustermap(\n",
    "    test_gene, \n",
    "    cmap=sns.diverging_palette(10, 220, n=256),\n",
    "    \n",
    "    figsize=(20,20)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tender-running",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ancient-lebanon",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_pc1_pc2(df, meta):\n",
    "    df = df.T\n",
    "    pca = PCA(n_components=2)\n",
    "    principalComponents = pca.fit_transform(df)\n",
    "    pDf = (pd.DataFrame(data=principalComponents, columns=['PC1', 'PC2'])\n",
    "           .set_index(df.index))\n",
    "\n",
    "    pc1_var = round(pca.explained_variance_ratio_[0] * 100, 2)\n",
    "    pc2_var = round(pca.explained_variance_ratio_[1] * 100, 2)\n",
    "    pDf2 = pDf.merge(meta, left_index=True, right_index=True)\n",
    "    return pDf2, pc1_var, pc2_var\n",
    "\n",
    "\n",
    "def plotPCA(pDf, pc1_var, pc2_var, colorby, col, nameby=\"\", el=False):\n",
    "    sns.set_style(\"ticks\")\n",
    "    sns.set_context(\"notebook\", font_scale=2.2)\n",
    "    group = pDf[colorby].unique()\n",
    "    assert len(group) <= len(col)\n",
    "    fig = plt.figure(figsize=(25, 15))\n",
    "    for g, c in zip(group, col):\n",
    "        df = pDf[pDf[colorby] == g]\n",
    "        x, y = df[[\"PC1\"]].values, df[[\"PC2\"]].values\n",
    "        ax = plt.scatter(x, y, c=c, s=150, label=g)\n",
    "        if el:\n",
    "            pts = np.asarray([[float(a), float(b)] for a, b in zip(x, y)])\n",
    "            plot_point_cov(pts, nstd=2, alpha=0.1, color=c)\n",
    "        if nameby:\n",
    "            labels = df[nameby]\n",
    "            for label, pc1, pc2 in zip(labels, x, y):\n",
    "                plt.annotate(label, xy=(pc1, pc2), xytext=(-5, 7), textcoords=\"offset points\",fontsize=14)\n",
    "        plt.xlabel('Principal Component 1, {} %'.format(pc1_var), )\n",
    "        plt.ylabel('Principal Component 2, {} %'.format(pc2_var), )\n",
    "        #plt.xticks(fontsize=16)\n",
    "        #plt.yticks(fontsize=16)\n",
    "        plt.legend(frameon=True)\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "representative-character",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.columns = [c[1] for c in test_df.columns]\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verified-possibility",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "level-syndication",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "new = vsts[0].sampleID.str.split(\"_\", expand=True)\n",
    "new.columns = ['mouse', 'day', 'dnaid', 'experiment']\n",
    "meta = pd.concat([vsts[0][['library', 'sampleID']], new], axis=1).set_index('sampleID')\n",
    "\n",
    "pDf, pc1_var, pc2_var = find_pc1_pc2(test_df, meta)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "female-pitch",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta[meta.day == 'd0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "announced-portsmouth",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "fig = plt.figure(figsize=(20,30))\n",
    "ax = sns.clustermap(\n",
    "    library_gene_fitness[[c for c in library_gene_fitness.columns if 'd0' not in c]].corr(), \n",
    "    \n",
    "    cmap=sns.diverging_palette(10, 220, n=256),\n",
    "    square=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trying-french",
   "metadata": {},
   "outputs": [],
   "source": [
    "library_gene_fitness.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unique-package",
   "metadata": {},
   "outputs": [],
   "source": [
    "good_samples_noskew = [s for s in good_samples if s not in skewed_samples and s not in no_inoc_samples]\n",
    "fits = []\n",
    "cis = []\n",
    "wt_fits = []\n",
    "ssa_ci = []\n",
    "resultsFitList = []\n",
    "resultsCIList=[]\n",
    "# Fitness Results\n",
    "for library in libraries:\n",
    "    print(library)\n",
    "    library_vst, library_barcode_fitness = get_barcode_fitness_by_library(cnt_df, library, good_samples_noskew, outdir, filter_below=1000)\n",
    "    library_gene_fitness = get_gene_fitness_by_library(library_barcode_fitness, annotation_df)\n",
    "    library_wt_fitness = get_wt_fitness_by_library(library_barcode_fitness, annotation_df, phenotype='wt')\n",
    "    library_ssa_fitness = get_wt_fitness_by_library(library_barcode_fitness, annotation_df, phenotype='ssaV_invG')\n",
    "    library_gene_ci = library_gene_fitness.set_index('ShortName').apply(lambda x: x / library_wt_fitness.median()[x.name]).reset_index()\n",
    "    library_ssa_ci = library_ssa_fitness.median()/library_wt_fitness.median()\n",
    "    meltGeneFit = melt_sampleID(library_gene_fitness, idVar=['ShortName'], value_name='fitness')\n",
    "    meltWtFit = melt_sampleID(library_wt_fitness, idVar=['barcode', 'phenotype', 'conc'], value_name='fitness')\n",
    "    meltGeneCI = melt_sampleID(library_gene_ci, idVar=['ShortName'], value_name='ci')\n",
    "    resultsFit = get_library_results(meltGeneFit, meltWtFit, library)\n",
    "    resultsCI = get_library_results_ci(meltGeneCI, library_ssa_ci, library)\n",
    "    \n",
    "    fits.append(meltGeneFit.assign(library=library))\n",
    "    cis.append(meltGeneCI.assign(library=library))\n",
    "    wt_fits.append(meltWtFit.assign(library=library))\n",
    "    resultsFitList.append(resultsFit)\n",
    "    resultsCIList.append(resultsCI)\n",
    "    ssa_ci.append(pd.DataFrame(library_ssa_ci, columns=['ssa_ci']).assign(library=library))\n",
    "    \n",
    "m2_fits_no_skew = pd.concat(fits)\n",
    "m2_ci_no_skew = pd.concat(cis)\n",
    "m2_wt_fits_no_skew = pd.concat(wt_fits)\n",
    "m2_results_fit_no_skew = pd.concat(resultsFitList)\n",
    "m2_results_ci_no_skew = pd.concat(resultsCIList)\n",
    "m2_ssa_ci_no_skew = pd.concat(ssa_ci)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "laughing-stability",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fits = []\n",
    "cis = []\n",
    "wt_fits = []\n",
    "ssa_ci = []\n",
    "resultsFitList = []\n",
    "resultsCIList=[]\n",
    "# Fitness Results\n",
    "for library in libraries:\n",
    "    print(library)\n",
    "    library_vst, library_barcode_fitness = get_barcode_fitness_by_library(cnt_df, library, good_samples, outdir, filter_below=1000)\n",
    "    library_gene_fitness = get_gene_fitness_by_library(library_barcode_fitness, annotation_df)\n",
    "    library_wt_fitness = get_wt_fitness_by_library(library_barcode_fitness, annotation_df, phenotype='wt')\n",
    "    library_ssa_fitness = get_wt_fitness_by_library(library_barcode_fitness, annotation_df, phenotype='ssaV_invG')\n",
    "    library_gene_ci = library_gene_fitness.set_index('ShortName').apply(lambda x: x / library_wt_fitness.median()[x.name]).reset_index()\n",
    "    library_ssa_ci = library_ssa_fitness.median()/library_wt_fitness.median()\n",
    "    meltGeneFit = melt_sampleID(library_gene_fitness, idVar=['ShortName'], value_name='fitness')\n",
    "    meltWtFit = melt_sampleID(library_wt_fitness, idVar=['barcode', 'phenotype', 'conc'], value_name='fitness')\n",
    "    meltGeneCI = melt_sampleID(library_gene_ci, idVar=['ShortName'], value_name='ci')\n",
    "    resultsFit = get_library_results(meltGeneFit, meltWtFit, library)\n",
    "    resultsCI = get_library_results_ci(meltGeneCI, library_ssa_ci, library)\n",
    "    \n",
    "    fits.append(meltGeneFit.assign(library=library))\n",
    "    cis.append(meltGeneCI.assign(library=library))\n",
    "    wt_fits.append(meltWtFit.assign(library=library))\n",
    "    resultsFitList.append(resultsFit)\n",
    "    resultsCIList.append(resultsCI)\n",
    "    ssa_ci.append(pd.DataFrame(library_ssa_ci, columns=['ssa_ci']).assign(library=library))\n",
    "    \n",
    "m2_fits_skew = pd.concat(fits)\n",
    "m2_ci_skew = pd.concat(cis)\n",
    "m2_wt_fits_skew = pd.concat(wt_fits)\n",
    "m2_results_fit_skew = pd.concat(resultsFitList)\n",
    "m2_results_ci_skew = pd.concat(resultsCIList)\n",
    "m2_ssa_ci_skew = pd.concat(ssa_ci)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "armed-postage",
   "metadata": {},
   "outputs": [],
   "source": [
    "m2_fits_no_skew.to_csv(Path(outdir)/\"21-07-m2_fits_no_skew\")\n",
    "m2_ci_no_skew.to_csv(Path(outdir)/\"21-07-m2_ci_no_skew\")\n",
    "m2_wt_fits_no_skew.to_csv(Path(outdir)/\"21-07-m2_wt_fits_no_skew\")\n",
    "m2_results_fit_no_skew.to_csv(Path(outdir)/\"21-07-m2_results_fit_no_skew\")\n",
    "m2_results_ci_no_skew.to_csv(Path(outdir)/\"21-07-m2_results_ci_no_skew\")\n",
    "m2_ssa_ci_no_skew.to_csv(Path(outdir)/\"21-07-m2_ssa_ci_no_skew\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dutch-scenario",
   "metadata": {},
   "outputs": [],
   "source": [
    "m2_fits_skew.to_csv(Path(outdir)/\"21-07-m2_fits_skew\")\n",
    "m2_ci_skew.to_csv(Path(outdir)/\"21-07-m2_ci_skew\")\n",
    "m2_wt_fits_skew.to_csv(Path(outdir)/\"21-07-m2_wt_fits_skew\")\n",
    "m2_results_fit_skew.to_csv(Path(outdir)/\"21-07-m2_results_fit_skew\")\n",
    "m2_results_ci_skew.to_csv(Path(outdir)/\"21-07-m2_results_ci_skew\")\n",
    "m2_ssa_ci_skew.to_csv(Path(outdir)/\"21-07-m2_ssa_ci_skew\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inner-laser",
   "metadata": {},
   "outputs": [],
   "source": [
    "m2_results_fit_no_skew.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frozen-saturn",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CI Results: \n",
    "library_ssa_fitness = get_wt_fitness_by_library(library_barcode_fitness, annotation_df, phenotype='ssaV_invG')\n",
    "\n",
    "meltSsaFit = melt_sampleID(library_ssa_fitness, idVar=['barcode', 'phenotype', 'conc'], value_name='fitness')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parallel-audit",
   "metadata": {},
   "outputs": [],
   "source": [
    "library_gene_fitness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "double-signal",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occasional-madrid",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "institutional-manitoba",
   "metadata": {},
   "outputs": [],
   "source": [
    "library_gene_fitness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "obvious-album",
   "metadata": {},
   "outputs": [],
   "source": [
    "meltGeneFit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satisfactory-carolina",
   "metadata": {},
   "outputs": [],
   "source": [
    "meltSsaFit[['sampleID', 'fitness']].set_index('sampleID')/meltWtFit[['sampleID', 'fitness']].set_index('sampleID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boring-camera",
   "metadata": {},
   "outputs": [],
   "source": [
    "meltSsaFit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "union-passage",
   "metadata": {},
   "outputs": [],
   "source": [
    "library_ssa_fitness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alpine-intervention",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = meltGeneFit[['ShortName', 'day']].drop_duplicates()\n",
    "y[y.day !='d0'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "favorite-remove",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = results.merge(meltGeneFit[['ShortName', 'day']].drop_duplicates(), on=['ShortName', 'day'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "developmental-enhancement",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expensive-particle",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_dfs = []\n",
    "gene_fit_dfs = []\n",
    "ci_dfs = []\n",
    "res_dfs = []\n",
    "wt_fit_dfs = []\n",
    "ssa_ci_dfs = []\n",
    "\n",
    "for library in ['library_10_2']:\n",
    "    print(library)\n",
    "    lib_df = cnt_df[cnt_df.library == library].copy()\n",
    "    #Removing some noise\n",
    "    lib_df = lib_df[~((lib_df.libcnt.isna()) & (lib_df.phenotype.isna()))]\n",
    "    library_samples = [s for s in good_samples if s in lib_df.sampleID.unique() and s not in skewed_samples]\n",
    "    sdf, edf, design = generate_DE_dataset(lib_df, library_samples, sample_id='sampleID', filter_below=1000)\n",
    "    _, vst_df = get_fitness_results(outdir, library.replace(\"_\", \"-\"), '', sdf, edf, design)\n",
    "    method2_results = method2_analysis2(vst_df, annotation_df, library_samples, sample_id='sampleID', hits=0.05)\n",
    "    \n",
    "    for df in method2_results:\n",
    "        if not df.empty:\n",
    "            df['library'] = library\n",
    "    all_fitness_df, gene_fitness_df, ci_df, results_df, wt_fitness_df, ssa_ci_df = method2_results    \n",
    "    fit_dfs.append(all_fitness_df)\n",
    "    gene_fit_dfs.append(gene_fitness_df)\n",
    "    ci_dfs.append(ci_df)\n",
    "    res_dfs.append(results_df)\n",
    "    wt_fit_dfs.append(wt_fitness_df)\n",
    "    ssa_ci_dfs.append(ssa_ci_df)\n",
    "    \n",
    "fit2 = pd.concat(fit_dfs)\n",
    "fit2_gene = pd.concat(gene_fit_dfs)\n",
    "wt_fit2 = pd.concat(wt_fit_dfs)\n",
    "ssa_ci2 = pd.concat(ssa_ci_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "descending-biotechnology",
   "metadata": {},
   "outputs": [],
   "source": [
    "res2 = pd.concat(res_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "about-original",
   "metadata": {},
   "outputs": [],
   "source": [
    "res2.sample(5, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "natural-winner",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.groupby('day').padj.count()\n",
    "#print('Tested 1888 genes/barcodes')\n",
    "for day in ['d1', 'd2', 'd3', 'd4']:\n",
    "    print(f'Number of significant hits on {day}: {res2[(res2.day == day)&(res2.ci_padj < 0.05)].shape[0]}')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mathematical-hanging",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_control_df(fitness, phenotype='wt'):\n",
    "    fitness.columns = [c.replace(\"unenriched_\", \"unenriched-\") for c in fitness.columns]\n",
    "    fitness = fitness.drop(['day'], axis=1)\n",
    "\n",
    "    wt = fitness[fitness.phenotype == phenotype].dropna(axis=1).drop(['inoculum'], axis=1)\n",
    "    wt = wt.melt(id_vars=['barcode', 'phenotype', 'conc', 'library'], var_name='sampleID', value_name='fitness')\n",
    "    new = wt.sampleID.str.split(\"_\", expand=True)\n",
    "    new.columns = ['mouse', 'day', 'dnaid', 'experiment']\n",
    "    wt = wt.merge(new, left_index=True, right_index=True)\n",
    "    return wt\n",
    "\n",
    "wt = get_control_df(all_fitness_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occupied-associate",
   "metadata": {},
   "outputs": [],
   "source": [
    "wt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tamil-china",
   "metadata": {},
   "outputs": [],
   "source": [
    "day = 'd3'\n",
    "wt_d1 = wt[wt.day == day]\n",
    "fig = px.box(wt_d1, x='mouse', y=np.log2(wt_d1['fitness']), color='mouse',  hover_data=['conc', 'fitness'],\n",
    "        template='simple_white', title = f'WT-{day}',\n",
    "              labels={\"y\": \"log2(Fitness)\",\n",
    "                     \"conc\": \"Dilution\", \"fitness\": \"Fitness\"})\n",
    "fig.add_hline(y=0, line_width=3, line_dash=\"dash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "governmental-commons",
   "metadata": {},
   "outputs": [],
   "source": [
    "wt_fit2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "natural-karma",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_skewed = wt.groupby('sampleID').agg({'fitness': [lambda x: x.quantile(0.25),  lambda x: x.quantile(0.75)]}).reset_index()\n",
    "filter_skewed.columns = ['sampleID', 'lowQ', 'highQ']\n",
    "filter_skewed = filter_skewed[~(filter_skewed.lowQ<1.1)&(filter_skewed.highQ>0.9)]\n",
    "to_drop = filter_skewed.sampleID.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "olive-upper",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cultural-completion",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intermediate-corrections",
   "metadata": {},
   "outputs": [],
   "source": [
    "res2.loc['rfaI']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valid-entrance",
   "metadata": {},
   "outputs": [],
   "source": [
    "wt_fit2.groupby('day').wt.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wrapped-sound",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gene_ranksums(gene_values, wt_values):\n",
    "    return ranksums(gene_values, wt_values)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advanced-pollution",
   "metadata": {},
   "outputs": [],
   "source": [
    "gv = ci2[(ci2.ShortName == 'rfaI') & (ci2.day == 'd4')].CI.values\n",
    "wv = ssa_ci2.loc[ssa_ci2.day == 'd4'].CI.values\n",
    "gene_ranksums(gv, wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "freelance-healing",
   "metadata": {},
   "outputs": [],
   "source": [
    "res2.loc['rfaI']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subject-conducting",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Day 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "threaded-chester",
   "metadata": {},
   "outputs": [],
   "source": [
    "res1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "configured-millennium",
   "metadata": {},
   "outputs": [],
   "source": [
    "t.day.str.split(\"_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "different-kingdom",
   "metadata": {},
   "outputs": [],
   "source": [
    "ivars = ['gene', 'locus', 'num_barcodes', 'library', 'barcode', 'sstart', 'sseqid']\n",
    "vvars = ['num_samples', 'fitness_mean', 'fitness_std', 'ci', 'zscore', 'pval', 'padj']\n",
    "df_list = []\n",
    "for v in vvars:\n",
    "    t = res1.reset_index().melt(id_vars = ivars, value_vars=[c for c in res1.columns if v in c], value_name=v, var_name='day')\n",
    "    t['day'] = t.day.str.split(\"_\", expand=True)[0]\n",
    "    df_list.append(t)\n",
    "res1m = pd.concat(df_list, axis=1)\n",
    "res1m = res1m.loc[:, ~res1m.columns.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sexual-discount",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deadly-malpractice",
   "metadata": {},
   "outputs": [],
   "source": [
    "def significant()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "substantial-armenia",
   "metadata": {},
   "outputs": [],
   "source": [
    "res1m.groupby(['library', 'day']).agg({'padj': [significant]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rolled-board",
   "metadata": {},
   "outputs": [],
   "source": [
    "res2.groupby(['library', 'day']).agg({'ci_padj': [significant]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dynamic-hello",
   "metadata": {},
   "outputs": [],
   "source": [
    "res1m[(res1m.library == 'library_10_2') & (res1m.day=='d1')].sort_values('padj').head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fallen-period",
   "metadata": {},
   "outputs": [],
   "source": [
    "wt_fit2[wt_fit2.library == 'library_11_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nuclear-tiger",
   "metadata": {},
   "outputs": [],
   "source": [
    "for library in ['library_10_2']:\n",
    "    print(library)\n",
    "    lib_df = cnt_df[cnt_df.library == library].copy()\n",
    "    library_samples = [s for s in good_samples if s in lib_df.sampleID.unique()]\n",
    "    vst_df = run_VST_transformation(lib_df, library.replace(\"_\", '-'), good_samples, \n",
    "                                    outdir, sample_id='sampleID').set_index('barcode')\n",
    "    method2_results = method2_analysis(vst_df, annotation_df, library_samples, sample_id='sampleID', hits=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bridal-commitment",
   "metadata": {},
   "outputs": [],
   "source": [
    "vst_df.sample(5, random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fluid-occasions",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_fitness_df, gene_fitness_df, ci_df, results_df, wt_fitness_df, ssa_ci_df = method2_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "native-engagement",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.sample(5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "magnetic-reservoir",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.ci_hits.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cellular-opening",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collective-message",
   "metadata": {},
   "outputs": [],
   "source": [
    "vst_test = cnt_df[(cnt_df.library=='library_10_2') & (cnt_df.day == 'd1')][['barcode', 'cnt', 'sampleID']].drop_duplicates()\n",
    "\n",
    "vst_test = vst_test.set_index('barcode').pivot(columns='sampleID').fillna(0)\n",
    "vst_test.columns = [c[1] for c in vst_test.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confidential-discovery",
   "metadata": {},
   "outputs": [],
   "source": [
    "vst_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "identified-cooler",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(x=vst_test.mean(axis=1), y=vst_test.var(axis=1)/vst_test.mean(axis=1), log_x=True, log_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "apart-closer",
   "metadata": {},
   "outputs": [],
   "source": [
    "step1 = np.log(vst_test)\n",
    "step2 = step1.mean(axis=1)\n",
    "step3 = step2.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "\n",
    "step4 = step1.T.apply(lambda x: x - x.mean()).T.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "step5 = step4.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "juvenile-trigger",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaling_factors = step5.apply(math.exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optional-lodging",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaling_factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optical-dragon",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaling_factors['ad926_d1_dnaid2017_TV4592A']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complicated-essence",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_vst = vst_test.apply(lambda x: x/scaling_factors[x.name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dynamic-certificate",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "math.exp(7.69)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "photographic-count",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(x=norm_vst.mean(axis=1), y=norm_vst.var(axis=1)/vst_test.mean(axis=1), log_x=True, log_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stable-default",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
