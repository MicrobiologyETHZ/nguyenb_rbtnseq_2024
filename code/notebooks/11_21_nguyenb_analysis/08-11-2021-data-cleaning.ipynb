{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-24T14:02:57.640099Z",
     "start_time": "2021-11-24T14:02:57.627638Z"
    }
   },
   "outputs": [],
   "source": [
    "# %load ../snippets/basic_settings.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "sns.set_context(\"notebook\", font_scale=1.1)\n",
    "pd.set_option(\"display.max_columns\", 100)\n",
    "pd.set_option(\"display.max_rows\", 100)\n",
    "plt.rcParams[\"figure.figsize\"] = (16, 12)\n",
    "plt.rcParams['savefig.dpi'] = 200\n",
    "plt.rcParams['figure.autolayout'] = False\n",
    "plt.rcParams['axes.labelsize'] = 18\n",
    "plt.rcParams['axes.titlesize'] = 20\n",
    "plt.rcParams['font.size'] = 16\n",
    "plt.rcParams['lines.linewidth'] = 2.0\n",
    "plt.rcParams['lines.markersize'] = 8\n",
    "plt.rcParams['legend.fontsize'] = 14\n",
    "plt.rcParams['font.family'] = \"serif\"\n",
    "plt.rcParams['font.serif'] = \"cm\"\n",
    "pd.set_option('display.float_format', lambda x: '{:,.2f}'.format(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-24T14:02:58.163826Z",
     "start_time": "2021-11-24T14:02:58.159373Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import datetime\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing for Bidongs RB-TNseq screen  \n",
    "\n",
    "## Loading the data\n",
    " - Need count files generated by `tnseq2`\n",
    " - `control.txt` lists all the control barcodes, their phenotypes and concentrations\n",
    " - `metadata.tsv` provides metatdata for each sample (library, experiment, dnaid, mouse, day, tissue)\n",
    " - `mapped.csv` files will contain mapped barcodes, `unmapped.csv` files will contain control barcodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-24T14:02:59.232319Z",
     "start_time": "2021-11-24T14:02:59.226571Z"
    }
   },
   "outputs": [],
   "source": [
    "root = Path(\"/nfs/nas22/fs2202/biol_micro_bioinf_nccr/hardt/nguyenb/tnseq\")\n",
    "dataDir = root/\"scratch/08_21/counts/\"\n",
    "controls_file = root/\"data/metadata/controls.txt\"\n",
    "outDir = root/\"scratch/08_21/results/nguyenb\"\n",
    "metafile = root/\"scratch/08_21/complete_metadata.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-24T14:02:59.602743Z",
     "start_time": "2021-11-24T14:02:59.566099Z"
    }
   },
   "outputs": [],
   "source": [
    "files = [f for f in dataDir.glob(\"*/*_mapped.csv\")]\n",
    "files_unmapped = [f for f in dataDir.glob(\"*/*_unmapped.csv\")]\n",
    "metadata = pd.read_table(metafile,index_col=0, header=None)\n",
    "metadata.columns = [\"library\", \"experiment\", \"mouse\", \"day\", \"tissue\", \"dnaid\", \"sampleID\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make sure using files from Nov 4-5/2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-24T14:03:00.451324Z",
     "start_time": "2021-11-24T14:03:00.346582Z"
    },
    "code_folding": [
     2
    ]
   },
   "outputs": [],
   "source": [
    "start_date = datetime.date(2021, 11, 4)\n",
    "end_date = datetime.date(2021, 11, 5)\n",
    "for file in files:\n",
    "    try:\n",
    "        assert start_date <= datetime.datetime.fromtimestamp(os.path.getmtime(file)).date() <= end_date \n",
    "    except AssertionError:\n",
    "        print(f'{file.name}: Date modified is not the expected date')\n",
    "        print(\"last modified: %s\" % datetime.datetime.fromtimestamp(os.path.getmtime(file)).date())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-24T14:03:14.525836Z",
     "start_time": "2021-11-24T14:03:00.704847Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.concat([pd.read_csv(f, index_col=0).assign(sampleID=f.stem.split('_counts')[0]) for f in files])\n",
    "df = df.merge(metadata, how='left', on='sampleID').dropna(subset=['mouse'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the control barcodes out of the unmapped files\n",
    "\n",
    "- Somewhat complicated way of getting control counts for all samples, can use some refactoring\n",
    "- Potentially something that `mBARq` can output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-24T14:03:27.457794Z",
     "start_time": "2021-11-24T14:03:14.528884Z"
    }
   },
   "outputs": [],
   "source": [
    "df_unmapped = pd.concat([pd.read_csv(f, index_col=0).assign(sampleID=f.stem.split('_counts')[0]) for f in files_unmapped])\n",
    "df_unmapped = df_unmapped.merge(metadata, how='left', on='sampleID').dropna(subset=['mouse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-24T14:03:27.586079Z",
     "start_time": "2021-11-24T14:03:27.459982Z"
    }
   },
   "outputs": [],
   "source": [
    "controls = pd.read_table(controls_file, header=None, index_col=0, names=['barcode', 'phenotype', 'conc'])\n",
    "# Create zero df\n",
    "a = np.zeros(shape=(controls.shape[0], df.sampleID.nunique()))\n",
    "zdf = (pd.DataFrame(a,columns=list(df.sampleID.unique()))\n",
    ".set_index(controls.barcode.values)\n",
    ".reset_index()\n",
    ".rename({'index':'barcode'}, axis=1))\n",
    "zdf = (zdf.melt(id_vars=['barcode'],var_name='sampleID', value_name='zero_cnt')\n",
    "       .merge(metadata, on='sampleID')\n",
    "      .drop('zero_cnt', axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-24T14:03:30.079809Z",
     "start_time": "2021-11-24T14:03:27.588668Z"
    }
   },
   "outputs": [],
   "source": [
    "# Merge zdf with controls to make sure all barcodes are 'present' for each sample\n",
    "# Then merge with unmapped, convert na to 0\n",
    "controls = controls.merge(zdf, on=['barcode'])\n",
    "controls = controls.merge(df_unmapped[['barcode','sampleID', 'barcode_cnt']], how='left', on=['barcode','sampleID'])\n",
    "controls['barcode_cnt'] = controls['barcode_cnt'].fillna(0)\n",
    "controls['ShortName'] = controls['phenotype'] + '-' + controls['conc'].astype(str)\n",
    "df = pd.concat([df, controls])\n",
    "control_barcodes = controls.barcode.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-24T14:03:30.095671Z",
     "start_time": "2021-11-24T14:03:30.082024Z"
    }
   },
   "outputs": [],
   "source": [
    "# should have 30 barcodes for each sample\n",
    "controls.groupby(['sampleID']).barcode.nunique().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quality Control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating WITS correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-24T14:03:33.198529Z",
     "start_time": "2021-11-24T14:03:33.186620Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_correlation(controls, concentration_col = 'conc', cnt_col='barcode_cnt',\n",
    "                          phenotype_col='phenotype', wt_phenotype='wt',\n",
    "                          for_each='sampleID', cutoff=0.8):\n",
    "    \"\"\"\n",
    "\n",
    "    Calculate correlation on log (counts+1) \n",
    "    Return control_cnts dataframe: contains all the metadata, logCnts, logConc, R, R2 for all the control barcodes\n",
    "    \"\"\"\n",
    "    control_cnts = controls.copy()\n",
    "    control_cnts['logConc'] = np.log10(control_cnts[concentration_col])\n",
    "    control_cnts['logCnts'] = np.log10(control_cnts[cnt_col]+1)\n",
    "    corr_df = control_cnts.groupby([phenotype_col, for_each])[['logConc', 'logCnts']].corr()\n",
    "    corr_df = corr_df.reset_index()\n",
    "    corr_df = corr_df[corr_df['level_2'] == 'logConc'].drop(['level_2', 'logConc'], axis=1)\n",
    "    corr_df.columns = [phenotype_col, for_each, 'R']\n",
    "    control_cnts = control_cnts.merge(corr_df, on = [for_each, phenotype_col])\n",
    "    control_cnts['R2'] = control_cnts.R**2\n",
    "    good_samples = control_cnts[(control_cnts.R2 > cutoff) & (control_cnts.phenotype == wt_phenotype)][for_each].unique()\n",
    "    return control_cnts, good_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-24T14:03:34.486376Z",
     "start_time": "2021-11-24T14:03:33.748474Z"
    }
   },
   "outputs": [],
   "source": [
    "control_cnts, good_samples = calculate_correlation(controls, concentration_col = 'conc',\n",
    "                          cnt_col='barcode_cnt', phenotype_col='phenotype',\n",
    "                          for_each='sampleID',  wt_phenotype='wt', cutoff=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-17T10:17:08.314499Z",
     "start_time": "2021-11-17T10:17:08.303815Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def graph_wits_correlation(control_cnts, sampleName, ax, phenotype='wt', label_cols = ('mouse', 'day', 'experiment')):\n",
    "    meta_dict = control_cnts.set_index('sampleID').to_dict()\n",
    "    df = control_cnts[(control_cnts.phenotype == phenotype) & (control_cnts.sampleID == sampleName)].copy()\n",
    "    r2 = round(df.R2.unique()[0], 2)\n",
    "    label = \"_\".join([meta_dict[c][sampleName] for c in label_cols]) + f\"; R2 = {r2}\"\n",
    "    sns.regplot(data=df, x='logConc', y='logCnts', ax=ax, x_jitter=0.05, color='black');\n",
    "    ax.set_title(label);\n",
    "    ax.set_xlim(df.logConc.min() -0.5, df.logConc.max() + 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-17T10:21:29.244019Z",
     "start_time": "2021-11-17T10:18:58.655224Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "libraries = df.library.unique()\n",
    "for library in libraries:\n",
    "    lib_control = control_cnts[control_cnts.library == library]\n",
    "    nrows = math.ceil(lib_control.sampleID.nunique()/4)\n",
    "    samples = (lib_control[['mouse', 'experiment', 'day', 'sampleID']]\n",
    "     .drop_duplicates()\n",
    "     .sort_values(['mouse', 'day'])\n",
    "     .sampleID.values)\n",
    "    sns.set(font_scale=1.1)\n",
    "    sns.set_style('whitegrid')\n",
    "    fig, axes = plt.subplots(nrows, 4, figsize=(20, nrows*4))\n",
    "    axes = axes.flatten()\n",
    "    for i, name in enumerate(samples):\n",
    "        graph_wits_correlation(lib_control, name, axes[i])\n",
    "        axes[i].set_ylim(-0.5, 4.5);\n",
    "    plt.subplots_adjust(left=0.125, bottom=0.1, right=0.9, top=0.9, wspace=0.2, hspace=0.5)\n",
    "    fig.savefig(outDir/f\"controls/{library}_WT_correlations.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-17T10:23:11.405172Z",
     "start_time": "2021-11-17T10:23:11.396626Z"
    }
   },
   "outputs": [],
   "source": [
    "control_cnts.phenotype.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-17T10:54:46.278022Z",
     "start_time": "2021-11-17T10:52:09.914941Z"
    }
   },
   "outputs": [],
   "source": [
    "libraries = df.library.unique()\n",
    "for library in libraries:\n",
    "    lib_control = control_cnts[control_cnts.library == library]\n",
    "    nrows = math.ceil(lib_control.sampleID.nunique()/4)\n",
    "    samples = (lib_control[['mouse', 'experiment', 'day', 'sampleID']]\n",
    "     .drop_duplicates()\n",
    "     .sort_values(['mouse', 'day'])\n",
    "     .sampleID.values)\n",
    "    sns.set(font_scale=1.1)\n",
    "    sns.set_style('whitegrid')\n",
    "    fig, axes = plt.subplots(nrows, 4, figsize=(20, nrows*4))\n",
    "    axes = axes.flatten()\n",
    "    for i, name in enumerate(samples):\n",
    "\n",
    "        graph_wits_correlation(lib_control, name, axes[i], phenotype = 'hyb',)\n",
    "        axes[i].set_ylim(-0.5, 4.5);\n",
    "    plt.subplots_adjust(left=0.125, bottom=0.1, right=0.9, top=0.9, wspace=0.2, hspace=0.5)\n",
    "    fig.savefig(outDir/f\"controls/{library}_hyb_correlations.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-17T10:57:29.124533Z",
     "start_time": "2021-11-17T10:54:46.281106Z"
    }
   },
   "outputs": [],
   "source": [
    "libraries = df.library.unique()\n",
    "for library in libraries:\n",
    "    lib_control = control_cnts[control_cnts.library == library]\n",
    "    nrows = math.ceil(lib_control.sampleID.nunique()/4)\n",
    "    samples = (lib_control[['mouse', 'experiment', 'day', 'sampleID']]\n",
    "     .drop_duplicates()\n",
    "     .sort_values(['mouse', 'day'])\n",
    "     .sampleID.values)\n",
    "    sns.set(font_scale=1.1)\n",
    "    sns.set_style('whitegrid')\n",
    "    fig, axes = plt.subplots(nrows, 4, figsize=(20, nrows*4))\n",
    "    axes = axes.flatten()\n",
    "    for i, name in enumerate(samples):\n",
    "\n",
    "        graph_wits_correlation(lib_control, name, axes[i], phenotype = 'chey',)\n",
    "        axes[i].set_ylim(-0.5, 4.5);\n",
    "    plt.subplots_adjust(left=0.125, bottom=0.1, right=0.9, top=0.9, wspace=0.2, hspace=0.5)\n",
    "    fig.savefig(outDir/f\"controls/{library}_chey_correlations.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-17T10:59:56.703966Z",
     "start_time": "2021-11-17T10:57:29.126468Z"
    }
   },
   "outputs": [],
   "source": [
    "libraries = df.library.unique()\n",
    "for library in libraries:\n",
    "    lib_control = control_cnts[control_cnts.library == library]\n",
    "    nrows = math.ceil(lib_control.sampleID.nunique()/4)\n",
    "    samples = (lib_control[['mouse', 'experiment', 'day', 'sampleID']]\n",
    "     .drop_duplicates()\n",
    "     .sort_values(['mouse', 'day'])\n",
    "     .sampleID.values)\n",
    "    sns.set(font_scale=1.1)\n",
    "    sns.set_style('whitegrid')\n",
    "    fig, axes = plt.subplots(nrows, 4, figsize=(20, nrows*4))\n",
    "    axes = axes.flatten()\n",
    "    for i, name in enumerate(samples):\n",
    "\n",
    "        graph_wits_correlation(lib_control, name, axes[i], phenotype = 'ssaV_invG',)\n",
    "        axes[i].set_ylim(-0.5, 4.5);\n",
    "    plt.subplots_adjust(left=0.125, bottom=0.1, right=0.9, top=0.9, wspace=0.2, hspace=0.5)\n",
    "    fig.savefig(outDir/f\"controls/{library}_ssaV_invG_correlations.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarizing at gene level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping unenriched samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-24T14:03:55.863840Z",
     "start_time": "2021-11-24T14:03:55.087965Z"
    }
   },
   "outputs": [],
   "source": [
    "unenriched = metadata[metadata.mouse.str.contains('unenriched')].sampleID.values\n",
    "good_samples = [s for s in good_samples if s not in unenriched]\n",
    "print(len(good_samples))\n",
    "clean_df = df[df.sampleID.isin(good_samples)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summing barcodes for each ShortName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-24T14:04:24.170897Z",
     "start_time": "2021-11-24T14:04:23.890607Z"
    }
   },
   "outputs": [],
   "source": [
    "clean_gene_df = (clean_df.groupby(['library', 'sampleID', 'ShortName', 'experiment', 'mouse', 'day', 'tissue'])\n",
    "          .barcode_cnt.sum().reset_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writting out clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-24T14:15:11.142270Z",
     "start_time": "2021-11-24T14:15:10.259777Z"
    }
   },
   "outputs": [],
   "source": [
    "clean_gene_df.to_csv(outDir/'09-11-2021-annotated_gene_counts_after_qc.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarizing clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-24T14:12:10.499573Z",
     "start_time": "2021-11-24T14:12:10.394734Z"
    }
   },
   "outputs": [],
   "source": [
    "summary_table = clean_gene_df.groupby(['library', 'experiment', 'day']).agg({'sampleID':['nunique'], 'ShortName':['nunique']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-24T14:14:07.916318Z",
     "start_time": "2021-11-24T14:14:07.911637Z"
    }
   },
   "outputs": [],
   "source": [
    "summary_table.columns = ['Number of samples', 'Number of genes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-24T14:14:52.916125Z",
     "start_time": "2021-11-24T14:14:52.774144Z"
    }
   },
   "outputs": [],
   "source": [
    "summary_table.to_csv(outDir/'24-11-2021-summary-clean-samples.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288.767px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
