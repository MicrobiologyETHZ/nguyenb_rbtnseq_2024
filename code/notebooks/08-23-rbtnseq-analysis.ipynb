{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../snippets/basic_settings.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from pathlib import Path\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import plotly.express as px\n",
    "import yaml\n",
    "import pyranges as pr\n",
    "import plotly.io as pio\n",
    "import re\n",
    "\n",
    "sns.set_context(\"notebook\", font_scale=1.1)\n",
    "pd.set_option(\"display.max_columns\", 100)\n",
    "pd.set_option(\"display.max_rows\", 100)\n",
    "plt.rcParams[\"figure.figsize\"] = (16, 12)\n",
    "plt.rcParams['savefig.dpi'] = 200\n",
    "plt.rcParams['figure.autolayout'] = False\n",
    "plt.rcParams['axes.labelsize'] = 18\n",
    "plt.rcParams['axes.titlesize'] = 20\n",
    "plt.rcParams['font.size'] = 16\n",
    "plt.rcParams['lines.linewidth'] = 2.0\n",
    "plt.rcParams['lines.markersize'] = 8\n",
    "plt.rcParams['legend.fontsize'] = 14\n",
    "pd.set_option('display.float_format', lambda x: '{:,.2f}'.format(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"nguyenb_config.yaml\", \"r\") as fh:\n",
    "    config_dict = yaml.safe_load(fh)\n",
    "\n",
    "root= Path(config_dict['root'])\n",
    "out_dir = Path(config_dict['out_dir'])\n",
    "map_dir = root/config_dict['map_dir']\n",
    "analysis_dir = root/config_dict['analysis_dir']\n",
    "#analysis_dir_5bcs = Path(config_dict['analysis_dir_5bcs'])\n",
    "\n",
    "gff = pr.read_gff3(root/config_dict['gff_file'])\n",
    "cds = gff[gff.Feature == 'CDS'].as_df()\n",
    "# counts_dir = Path(config_dict['counts_dir'])\n",
    "sd = pd.read_csv(root/config_dict['sample_data_file'])\n",
    "sd['name'] = sd['mouse'] + \"_\" + sd['library'] + \"_\" + sd['day'] + \"_\"+ sd['dnaid']\n",
    "sd = sd.rename(columns={'sampleID':'sample_id'})\n",
    "\n",
    "\n",
    "alphabetClrs = px.colors.qualitative.Alphabet\n",
    "sushi_colors_dict = {'red': '#C0504D',\n",
    "                     'orange': '#F79646',\n",
    "                     'medSea': '#4BACC6',\n",
    "                     'black': '#000000',\n",
    "                     'dgreen': '#00B04E',\n",
    "                     'lgreen': '#92D050',\n",
    "                     'dblue': '#366092',\n",
    "                     'lblue': '#95B3D7',\n",
    "                     'grey': alphabetClrs[8]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maps = list(map_dir.rglob(\"*annotated.csv.gz\"))\n",
    "map_df = pd.concat([pd.read_csv(f).assign(library=f.stem.split(\".annotated\")[0]) for f in maps])\n",
    "map_sum = map_df.groupby('library').agg({'barcode':['nunique'], 'ID':['nunique']}).reset_index()\n",
    "map_sum.columns = ['library', 'num_inserts', 'num_genes']\n",
    "map_df['Library'] = map_df['library'].str.replace(\"library_\", '').str.replace('_', '.').astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_df.library.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_combined_site(insertion_sites, chrs):\n",
    "    \"\"\"\n",
    "    Computes combined insertion sites based on chromosome-specific offsets to help plot all of the on one axis.\n",
    "\n",
    "    Given a list of insertion sites and corresponding chromosome identifiers,\n",
    "    this function adjusts the insertion sites to account for the cumulative\n",
    "    lengths and predefined offsets of each chromosome. The result is a list\n",
    "    of combined insertion sites that can be used for unified genomic analysis.\n",
    "\n",
    "    Parameters:\n",
    "    insertion_sites (list of int): A list of insertion site positions.\n",
    "    chrs (list of str): A list of chromosome identifiers corresponding to the insertion sites.\n",
    "                        Valid chromosome identifiers are 'FQ312003.1', 'HE654724.1', \n",
    "                        'HE654725.1', and 'HE654726.1'.\n",
    "\n",
    "    Returns:\n",
    "    list of int: A list of combined insertion site positions adjusted according \n",
    "                 to chromosome lengths and offsets.\n",
    "\n",
    "    Example:\n",
    "    >>> insertion_sites = [1000, 2000, 3000, 4000]\n",
    "    >>> chrs = ['FQ312003.1', 'HE654724.1', 'HE654725.1', 'HE654726.1']\n",
    "    >>> get_combined_site(insertion_sites, chrs)\n",
    "    [1000, 5078012, 5161854, 5248762]\n",
    "\n",
    "    \"\"\"\n",
    "    chr_len = {'FQ312003.1': 4878012, \n",
    "               'HE654724.1': 93842,\n",
    "               'HE654725.1': 86908,\n",
    "               'HE654726.1': 8688}\n",
    "    new_sites = []\n",
    "    for s, c in zip(insertion_sites, chrs):\n",
    "        if c == 'FQ312003.1':\n",
    "            new_sites.append(s)\n",
    "        elif c == 'HE654724.1':\n",
    "            new_sites.append(s+100000+chr_len['FQ312003.1'])\n",
    "        elif c == 'HE654725.1':\n",
    "            new_sites.append(s+200000+chr_len['FQ312003.1'] + chr_len['HE654724.1'])\n",
    "        elif c == 'HE654726.1':\n",
    "             new_sites.append(s+300000+chr_len['FQ312003.1'] + chr_len['HE654724.1']+ chr_len['HE654725.1'])\n",
    "    return new_sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_df['new_sites'] = get_combined_site(map_df.insertion_site, map_df.chr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_df.groupby('library').agg({'barcode':['nunique'], 'locus_tag':['nunique']}).median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coverage plots\n",
    "\n",
    "- Uncomment #pio.write_image to save the figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prefix='map_coverage_log_11-1_highlight'\n",
    "\n",
    "fig = px.histogram(map_df.sort_values('Library'), x='new_sites', color='Library', nbins=200, \n",
    "             template='plotly_white', width=2000, height=500, color_discrete_sequence=[\"#FDBE10\"] + px.colors.sequential.deep, \n",
    "             labels={'new_sites': 'Position, bp'}, \n",
    "             category_orders={\"Library\": [11.1, 9.1, 10.1, 10.2,  11.2, 12.1, 12.2, 13.1, 13.2, 14.2, 15.1]},\n",
    "             #log_y=True, \n",
    "             )\n",
    "\n",
    "fig.update_layout(bargap=0.1, font=dict(size=20), font_family=\"Arial\")\n",
    "fig.update_xaxes(showline=True, linewidth=1, linecolor='black', \n",
    "                tickfont=dict(size=24, color='black'),  titlefont=dict(size=24, color='black'),\n",
    "                tickvals =[ 0, 1000000, 2000000, 3000000, 4000000])\n",
    "fig.update_yaxes(showline=True, linewidth=1, linecolor='black', title='Number of insertions',\n",
    "                 type=\"log\", \n",
    "                  tickvals =[ 0,10, 100, 1000],\n",
    "                tickfont=dict(size=20, color='black'), titlefont=dict(size=30, color='black'))\n",
    "fig.add_vline(4925000, line_width=1, line_dash=\"dash\", line_color=\"grey\")\n",
    "fig.add_vline(5125000, line_width=1, line_dash=\"dash\", line_color=\"grey\")\n",
    "fig.add_vline(5325000, line_width=1, line_dash=\"dash\", line_color=\"grey\")\n",
    "#pio.write_image(fig, out_dir/f\"{prefix}.svg\", width=2000, height=500, scale=2)\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix='map_coverage_log'\n",
    "fig = px.histogram(map_df.sort_values('Library'), x='new_sites', color='Library', nbins=200, \n",
    "             template='plotly_white', width=2000, height=500, color_discrete_sequence= px.colors.sequential.deep, \n",
    "             labels={'new_sites': 'Position, bp'}, \n",
    "             category_orders={\"Library\": [11.1, 9.1, 10.1, 10.2,  11.2, 12.1, 12.2, 13.1, 13.2, 14.2, 15.1]},\n",
    "             log_y=True, \n",
    "             )\n",
    "\n",
    "fig.update_layout(bargap=0.1, font=dict(size=20),font_family=\"Arial\")\n",
    "fig.update_xaxes(showline=True, linewidth=1, linecolor='black', \n",
    "                tickfont=dict(size=24, color='black'),  titlefont=dict(size=24, color='black'),\n",
    "                tickvals =[ 0, 1000000, 2000000, 3000000, 4000000])\n",
    "fig.update_yaxes(showline=True, linewidth=1, linecolor='black', title='Number of insertions',\n",
    "                 tickvals =[ 0,10, 100, 1000],\n",
    "                tickfont=dict(size=20, color='black'), titlefont=dict(size=30, color='black'))\n",
    "fig.add_vline(4925000, line_width=1, line_dash=\"dash\", line_color=\"grey\")\n",
    "fig.add_vline(5125000, line_width=1, line_dash=\"dash\", line_color=\"grey\")\n",
    "fig.add_vline(5325000, line_width=1, line_dash=\"dash\", line_color=\"grey\")\n",
    "#pio.write_image(fig, out_dir/f\"{prefix}.svg\", width=2000, height=500, scale=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number unique barcodes: {map_df.barcode.nunique()}\")\n",
    "print(f\"Number unique genes: {map_df.Name.nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of insertions accross libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_gene_summary = map_df.groupby('Name').library.nunique().reset_index()\n",
    "prefix = \"number_of_libraries_with_gene_disruption\"\n",
    "fig = px.histogram(map_gene_summary, x='library', \n",
    "             template='plotly_white', width=900, height=700, color_discrete_sequence=px.colors.sequential.gray, \n",
    "             labels={'insertion_site': 'Position, bp'}, log_y=False)\n",
    "\n",
    "fig.update_layout(bargap=0.1, font_family=\"Arial\")\n",
    "fig.update_xaxes(showline=True, linewidth=1, linecolor='black', title='Libraries containing the gene disruption', tickvals = [1,2,3,4,5,6,7,8,9,10,11],\n",
    "                tickfont=dict(size=20, color='black'),  titlefont=dict(size=24, color='black'))\n",
    "fig.update_yaxes(showline=True, linewidth=1, linecolor='black', title='Number of genes',\n",
    "                tickfont=dict(size=24, color='black'), titlefont=dict(size=30, color='black'),\n",
    "                range=[0, 1000],\n",
    "                tickvals=[0, 250, 500, 750, 1000])\n",
    "#pio.write_image(fig, out_dir/f\"{prefix}.svg\", width=900, height=700, scale=2)\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genes hit only once\n",
    "map_genes_once = set(map_gene_summary[map_gene_summary.library == 1].Name.unique())\n",
    "print(f\"Number of genes hit only once: {len(map_genes_once)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genes that were not hit at all\n",
    "annotated_genes = gff[gff.Feature == 'gene'].as_df()\n",
    "genes_with_no_hits = map_df[['Name',  'ID']].drop_duplicates().merge(annotated_genes[['Chromosome', 'Start', 'End', 'Strand', 'Name', 'locus_tag']], on=['Name'], how='outer')\n",
    "genes_with_no_hits = genes_with_no_hits[genes_with_no_hits.ID.isna()][['Chromosome', 'Start', 'End', 'Strand', 'Name', 'locus_tag']].sort_values(['Chromosome', 'Start'])\n",
    "#genes_with_no_hits.to_csv(out_dir/'genes_without_hits.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of genes disrupted by library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations \n",
    "import random\n",
    "\n",
    "def get_genes_disrupted(df, all_libs, r=2):\n",
    "    lib_combs = list(combinations(all_libs, r))\n",
    "    if len(lib_combs) > 1000:\n",
    "        lib_combs = random.choices(lib_combs, k=1000)\n",
    "    genes_with_disruptions = []\n",
    "    for c in lib_combs:\n",
    "        fdf = df[df.library.isin(c)].copy()\n",
    "        ngenes = fdf[fdf.distance_to_feature == 0].Name.nunique()\n",
    "        genes_with_disruptions.append(ngenes)\n",
    "    return genes_with_disruptions\n",
    "\n",
    "dfl = []\n",
    "all_libs = map_df.library.unique()\n",
    "for i in range(1,21):\n",
    "    genes = get_genes_disrupted(map_df, all_libs, i)\n",
    "    Ngenomes = [i]*len(genes)\n",
    "    dfl.append(pd.DataFrame([Ngenomes, genes]).T)\n",
    "num_genes_hit = pd.concat(dfl)\n",
    "num_genes_hit.columns = ['Number of libraries', 'Number of genes with an insertion']\n",
    "# sns.boxplot(data=num_genes_hit, x='Number of libraries', y='Number of genes with an insertion')\n",
    "# plt.axvline(x=9, color='grey', linestyle='--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_genes_hit['Number of libraries'] = num_genes_hit['Number of libraries'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of genes disrupted in library_11_1: {map_df[map_df.library == 'library_11_1'].Name.nunique()}\")\n",
    "print(f\"Number of genes disrupted in 11 libraryes: {map_df.Name.nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "font_size=24\n",
    "h=600\n",
    "w=750\n",
    "prefix='number_of_genes_with_insertion_over_libraries'\n",
    "fig = px.box(num_genes_hit, x='Number of libraries', y='Number of genes with an insertion', \n",
    "             color_discrete_sequence = ['black']*20, \n",
    "             height=h, width=w,  template='plotly_white')\n",
    "\n",
    "fig.update_xaxes(showline=True, linewidth=2, linecolor='black',\n",
    "                          tickfont=dict(size=font_size-6, color='black'), \n",
    "                  titlefont=dict(size=font_size, color='black'))\n",
    "fig.update_yaxes(showline=True, linewidth=2, linecolor='black', \n",
    "                         tickfont=dict(size=font_size-6, color='black'), \n",
    "                  titlefont=dict(size=font_size, color='black'), \n",
    "                  range=[0, 4000])\n",
    "#fig.add_vline(x=11, line_width=1, line_dash='dash', line_color=sushi_colors_dict['red'])\n",
    "fig.add_hline(y=853, line_width=2, line_dash='dash', line_color='black', annotation_text=\"853\")\n",
    "fig.add_hline(y=3076, line_width=2, line_dash='dash', line_color=sushi_colors_dict['red'], annotation_text=\"3076\", )\n",
    "fig.update_layout(showlegend=False, font_size=24, font_family=\"Arial\")\n",
    "#pio.write_image(fig, out_dir/f\"{prefix}.svg\", width=w, height=h, scale=2)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some experiments were conducted with 15 control barcodes, and some with 5, these had to be analyzed independently\n",
    "result_files = list((analysis_dir/\"15bc\").glob(\"*rra_results.csv.gz\"))\n",
    "result_files5 = list((analysis_dir/\"5bc\").glob(\"*rra_results.csv.gz\"))\n",
    "df_list = []\n",
    "for f in result_files:\n",
    "    df = pd.read_csv(f).assign(library=f.stem.split(\"_rra\")[0], num_barcodes=15)\n",
    "    df_list.append(df)\n",
    "\n",
    "for f in result_files5:\n",
    "    df = pd.read_csv(f).assign(library=f.stem.split(\"_rra\")[0], num_barcodes=5)\n",
    "    df_list.append(df)\n",
    "\n",
    "\n",
    "fdf = pd.concat(df_list).drop_duplicates()\n",
    "fdf = fdf[~fdf.locus_tag.str.contains(\":\")]\n",
    "fdf['padj'] = fdf[['neg_selection_fdr','pos_selection_fdr']].min(axis=1)\n",
    "\n",
    "\n",
    "def identify_hits(row):\n",
    "    \"\"\"\n",
    "    Identifies significant hits based on log fold change (LFC) and adjusted p-value (padj).\n",
    "\n",
    "    This function evaluates a row of data to determine if it represents a significant\n",
    "    hit. A significant hit is defined based on the following criteria:\n",
    "    - If the log fold change (LFC) is less than -1 and the adjusted p-value (padj) is less than 0.05, it is considered a negative hit.\n",
    "    - If the log fold change (LFC) is greater than 1 and the adjusted p-value (padj) is less than 0.05, it is considered a positive hit.\n",
    "    - Otherwise, it is not considered a significant hit.\n",
    "\n",
    "    Parameters:\n",
    "    row (pandas.Series): A row of data containing at least the columns 'LFC' and 'padj'.\n",
    "\n",
    "    Returns:\n",
    "    int: Returns -1 for a negative hit, 1 for a positive hit, and 0 for no significant hit.\n",
    "\n",
    "    Example:\n",
    "    >>> import pandas as pd\n",
    "    >>> data = {'LFC': [-1.5, 2.0, 0.5], 'padj': [0.01, 0.04, 0.2]}\n",
    "    >>> df = pd.DataFrame(data)\n",
    "    >>> df['hit'] = df.apply(identify_hits, axis=1)\n",
    "    >>> df\n",
    "       LFC  padj  hit\n",
    "    0 -1.5  0.01   -1\n",
    "    1  2.0  0.04    1\n",
    "    2  0.5  0.20    0\n",
    "    \"\"\"\n",
    "    if row['LFC'] < -1 and row['padj'] < 0.05:\n",
    "        return -1\n",
    "    elif row['LFC'] > 1 and row['padj'] < 0.05:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "fdf['hit'] = fdf.apply(identify_hits, axis=1)\n",
    "# Eleminate all genes that were not hits\n",
    "rsig = fdf[fdf.hit != 0]\n",
    "# How many unique libraries identify a specific gene as a hit (hit includes a direction)\n",
    "num_hits = rsig.groupby(['locus_tag', 'contrast', 'hit']).library.nunique().reset_index().rename(columns={'library': 'num_hit'})\n",
    "# Identify ambigious hits, i.e. hits that were sometimes negative and sometimes positive\n",
    "amb_hits = rsig.groupby(['locus_tag', 'contrast']).hit.value_counts(normalize=True)\n",
    "amb_hits = pd.DataFrame(amb_hits).rename(columns={'hit':'proportion'}).reset_index() \n",
    "# These were identified as negative (or positive) hits 100% of the time\n",
    "true_hits = amb_hits[amb_hits.proportion == 1]\n",
    "# Discrard the ones that are positive 50% of the time and negative 50% of the time\n",
    "amb_hits = amb_hits.query(\"proportion != 1 & proportion != 0.5\")\n",
    "# Pick the direction of the majority\n",
    "amb_hits = amb_hits.loc[amb_hits.groupby('locus_tag')['proportion'].transform(max) == amb_hits['proportion']]\n",
    "amb_hits = pd.concat([true_hits, amb_hits])\n",
    "rsig = rsig.rename(columns={'hit': 'library_specific_hit'}).merge(amb_hits, on=['locus_tag', 'contrast'],how='inner')\n",
    "rsig = rsig.merge(num_hits, on=['locus_tag', 'contrast', 'hit'], how='left')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table1 = fdf.groupby(['library', 'contrast']).agg({'locus_tag':['nunique'], 'hit':['sum'],\n",
    "                                          'LFC': ['median']}).reset_index()\n",
    "table1.columns = ['library', 'day', 'number_genes_passing_qc', 'number_hits', 'median_LFC']\n",
    "table1 = table1.pivot(index=['library'], columns=['day'], values = ['number_genes_passing_qc', 'number_hits']).fillna(0)    \n",
    "table1.columns = [f'{col_name}_{day}' for col_name in ['number_genes_passing_qc', 'number_hits'] for day in ['d1', 'd2', 'd3', 'd4']]                           \n",
    "table1 = table1.astype(int)\n",
    "\n",
    "mice_summary = pd.read_csv(root/config_dict['mice_file'], index_col=0)\n",
    "print(mice_summary.sum())\n",
    "table1 = table1.merge(mice_summary, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#table1.to_csv(out_dir/'table1_number_of_hits_per_library.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation between libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get correlations\n",
    "fdf['library_num_barcodes'] = fdf['library'] + '_' + fdf['num_barcodes'].astype(str)\n",
    "lib_col = 'library_num_barcodes'\n",
    "cor_df = (fdf[['locus_tag', 'contrast', 'LFC', lib_col]]\n",
    "          .pivot(index=['locus_tag', 'contrast'], columns=lib_col)\n",
    "          .reset_index()\n",
    "          .set_index('locus_tag')\n",
    "          .groupby('contrast')\n",
    "          .corr()\n",
    "          .reset_index())\n",
    "df_list = []\n",
    "for i, g in cor_df.groupby('contrast'):\n",
    "    df = g.drop(['level_1'], axis=1).set_index(['contrast', lib_col])\n",
    "    df = (df.mask(np.triu(np.ones(df.shape, dtype=np.bool_)))\n",
    "          .stack()\n",
    "          .rename_axis(('contrast', 'lib1', 'lib2'))\n",
    "          .reset_index()\n",
    "          .rename(columns={'LFC': 'R'}))\n",
    "    df_list.append(df)\n",
    "cor_df = pd.concat(df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "font_size=24\n",
    "w=400\n",
    "h=500\n",
    "prefix='correlations_between_libraries'\n",
    "cor_df['Day'] = cor_df.contrast.replace({'d1':'Day 1', 'd2':'Day 2', 'd3': 'Day 3', 'd4': 'Day 4'})\n",
    "fig = px.box(cor_df, x='Day', y='R', color='Day',\n",
    "                  color_discrete_map = {'Day 1': sushi_colors_dict['red'], \n",
    "                                           'Day 2': sushi_colors_dict['dgreen'], \n",
    "                                           'Day 3': sushi_colors_dict['dblue'], \n",
    "                                           'Day 4': sushi_colors_dict['orange']},\n",
    "                labels={\"Day\":'', 'R': \"Pearson's <i>r</i>\"},\n",
    "\n",
    "                  height=h, width=w,  template='plotly_white', hover_data=['lib1', 'lib2'])\n",
    "fig.update_layout(showlegend=False, font_family=\"Arial\")\n",
    "fig.update_xaxes(showline=True, linewidth=2, linecolor='black',\n",
    "                  tickfont=dict(size=font_size-6, color='black'), \n",
    "                  tickangle=-50,\n",
    "                 titlefont=dict(size=font_size, color='black'))\n",
    "fig.update_yaxes(showline=True, linewidth=2, linecolor='black', range=[0, 1],\n",
    "                        tickfont=dict(size=font_size-6, color='black'), \n",
    "                 titlefont=dict(size=font_size, color='black'))\n",
    "#pio.write_image(fig, out_dir/f\"{prefix}.svg\", width=w, height=h, scale=2)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show example for 2 libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_df = (fdf.query('(library_num_barcodes==\"library_10_2_15\" | library_num_barcodes == \"library_11_2_15\") ')[['locus_tag', 'LFC', 'library_num_barcodes', 'contrast']]\n",
    "        .pivot(index=['locus_tag', 'contrast'], columns='library_num_barcodes')\n",
    "        .dropna()\n",
    "        .reset_index())\n",
    "ex_df.columns = ['locus_tag','day', 'library_10_1', 'library_12_1']\n",
    "ex_df['Day'] = ex_df.day.replace({'d1':'Day 1', 'd2':'Day 2', 'd3': 'Day 3', 'd4': 'Day 4'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h=600\n",
    "w=800\n",
    "prefix='example_correlation_10_1_12_1'\n",
    "px.scatter(ex_df, x='library_10_1', y='library_12_1',color='Day', width=500, height=500, \n",
    "            template='plotly_white', trendline='ols' )\n",
    "\n",
    "\n",
    "\n",
    "fig = px.scatter(ex_df, x='library_10_1', y='library_12_1', color='Day', \n",
    "                     height=h, width=w,\n",
    "                     template = 'plotly_white', \n",
    "                     labels = {'library_10_1': 'LFC(library 10-1)', \n",
    "                               'library_12_1': 'LFC (library 12-1)'},\n",
    "                     color_discrete_map = {'Day 1': sushi_colors_dict['red'], \n",
    "                                           'Day 2': sushi_colors_dict['dgreen'], \n",
    "                                           'Day 3': sushi_colors_dict['dblue'], \n",
    "                                           'Day 4': sushi_colors_dict['orange']},\n",
    "                #hover_data=['locus_tag', 'gene'],\n",
    "                category_orders = {'Day':['Day 1', 'Day 2', 'Day 3','Day 4']}, trendline='ols')\n",
    "\n",
    "fig.update_traces(marker=dict(size=14, line=dict(width=1, color='DarkSlateGrey'), \n",
    "                                opacity=0.9),\n",
    "                    selector=dict(mode='markers'))\n",
    "fig.update_xaxes(showline=True, linewidth=2, linecolor='black',\n",
    "                        tickfont=dict(size=font_size-6, color='black'), \n",
    "                    titlefont=dict(size=font_size, color='black'), range=[-14,8])\n",
    "fig.update_yaxes(showline=True, linewidth=2, linecolor='black',\n",
    "                    tickfont=dict(size=font_size-6, color='black'), \n",
    "                    titlefont=dict(size=font_size, color='black'), range=[-14,8])\n",
    "\n",
    "fig.update_layout(legend=dict(font=dict(size=font_size-2)), \n",
    "                    legend_title=dict(font=dict(size=font_size)), font_family=\"Arial\")\n",
    "\n",
    "tr_line=[]\n",
    "for  k, trace  in enumerate(fig.data):\n",
    "        if trace.mode is not None and trace.mode == 'lines':\n",
    "            tr_line.append(k)\n",
    "print(tr_line)\n",
    "\n",
    "for id in tr_line:\n",
    "    fig.data[id].update(line_width=6)\n",
    "\n",
    "#pio.write_image(fig, out_dir/f\"{prefix}.svg\", width=w, height=h, scale=2)\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fdf.to_csv(out_dir/'final-results-all-libraries.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate final results table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate information \n",
    "rsig_agg = rsig[['locus_tag', 'contrast', 'hit', 'num_hit']].drop_duplicates().copy()\n",
    "num_hit = (fdf.groupby(['contrast', 'locus_tag'])\n",
    "           .agg({'library':['nunique'], 'LFC':['median'], 'padj':['min', 'max']})\n",
    "           .reset_index())\n",
    "num_hit.columns = ['contrast', 'locus_tag', 'num_lib', 'LFC_median', 'padj_min', 'padj_max']\n",
    "num_hit =  (num_hit.merge(rsig_agg, on=['contrast', 'locus_tag'], how='outer'))\n",
    "num_hit['num_hit'] = num_hit['num_hit'].fillna(0).astype(int)\n",
    "num_hit['hit'] = num_hit['hit'].fillna(0).astype(int)\n",
    "num_hit['Detection frequency'] = num_hit['num_hit'].astype(str) + \"/\" + num_hit['num_lib'].astype(str)\n",
    "num_hit.columns = ['day', 'Name', 'number_of_libraries_with_mutant', 'LFC_median', 'padj_min', 'padj_max',  'hit','number_of_times_detected_as_hit', 'Detection frequency']\n",
    "num_hit['padj_min_formatted'] = num_hit['padj_min'].apply(lambda x: '< 0.01' if x < 0.01 else str(round(x, 2)))\n",
    "num_hit['padj_max_formatted'] = num_hit['padj_max'].apply(lambda x: '< 0.01' if x < 0.01 else str(round(x, 2)))\n",
    "num_hit['padj_range'] = num_hit['padj_min_formatted'] + \" - \" + num_hit['padj_max_formatted']\n",
    "num_hit['FC'] = 2**num_hit['LFC_median']\n",
    "num_hit['FC_with_detection_frequency'] = num_hit['FC'].round(2).astype(str) + \" (\" + num_hit['Detection frequency'] + \")\"\n",
    "num_hit['FC'] = num_hit[\"FC\"].round(4) \n",
    "num_hit_short = num_hit[['Name', 'day', 'FC_with_detection_frequency', 'padj_range', 'FC', 'LFC_median', 'Detection frequency', 'padj_min_formatted', 'padj_max_formatted', 'padj_min', 'padj_max', 'hit']].sort_values(['day','LFC_median'])\n",
    "\n",
    "only_once = num_hit_short[num_hit_short.Name.isin(map_genes_once)]\n",
    "final_summary_only_once =  only_once.pivot(index=['Name'], columns='day', \n",
    "              values=['FC_with_detection_frequency', 'padj_range', 'FC', 'LFC_median', 'padj_min_formatted', 'padj_max_formatted', 'padj_min', 'padj_max', 'hit']).reset_index()\n",
    "final_summary = num_hit_short.pivot(index=['Name'], columns='day', \n",
    "              values=['FC_with_detection_frequency', 'padj_range', 'FC', 'LFC_median', 'padj_min_formatted', 'padj_max_formatted', 'padj_min', 'padj_max', 'hit']).reset_index()\n",
    "col_names = [f'{col_name}_{day}' for col_name in ['FC_with_detection_frequency', 'padj_range', 'FC', 'LFC_median', 'padj_min_formatted', 'padj_max_formatted', 'padj_min', 'padj_max', 'hit'] for day in ['d1', 'd2', 'd3', 'd4']]\n",
    "final_summary.columns = ['Name'] + col_names\n",
    "final_summary_only_once.columns = ['Name'] + col_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_summary_only_once = final_summary_only_once[['Name'] + 'FC_d1\tFC_d2\tFC_d3\tFC_d4'.split() + \"padj_min_formatted_d1\tpadj_min_formatted_d2\tpadj_min_formatted_d3\tpadj_min_formatted_d4\".split()]\n",
    "final_summary_only_once.columns = [c.replace(\"_min_formatted\", '') for c in final_summary_only_once.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_summary_only_once = final_summary_only_once[[\"Name\", \"FC_d1\", \"padj_d1\", \"FC_d2\", \"padj_d2\", \"FC_d3\", \"padj_d3\", \"FC_d4\", \"padj_d4\"]].sort_values(\"Name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final_summary_only_once.to_csv(out_dir/'final-results-gene-level-summary-only-once.csv', index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_summary = final_summary.merge(annotated_genes[['Name', 'Chromosome']].drop_duplicates(), on='Name', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "late = final_summary.dropna(subset=['hit_d3', 'hit_d4']).query(\"hit_d1 == 0 & hit_d2 == 0 & hit_d3 != 0 & hit_d4 != 0 & hit_d3 == hit_d4\" )\n",
    "#late.to_csv(out_dir/'final-results-late-hits-only.csv', index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_summary_chr = final_summary[final_summary.Chromosome == 'FQ312003.1'].drop(columns=['Chromosome'])\n",
    "#final_summary_chr.to_csv(out_dir/'final-results-gene-level-summary-chromosome.csv', index=0)\n",
    "final_summary_plasmid = final_summary[final_summary.Chromosome != 'FQ312003.1'].drop(columns=['Chromosome']).drop_duplicates()\n",
    "#final_summary_plasmid.to_csv(out_dir/'final-results-gene-level-summary-plasmid.csv', index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final_summary.to_csv(out_dir/'final-results-gene-level-summary.csv', index=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functional annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load string annotation and merge with gff\n",
    "cds_short = cds[['Name', 'gene', 'locus_tag', 'product']].copy()\n",
    "cds_short['gene'] = cds_short['gene'].fillna(cds_short['locus_tag'])\n",
    "cds_short['Name'] = cds_short[\"Name\"].str.split(\".\", expand=True)[0]\n",
    "string = pd.read_table(root/config_dict['string_enrichment_file'])\n",
    "string['Name'] = string['#string_protein_id'].str.split(\".\", expand=True)[1] \n",
    "anot = string.merge(cds_short, on='Name', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset annotation to only include hits\n",
    "hits = list(rsig.locus_tag.unique())\n",
    "hits_anot = anot[(anot.gene.isin(hits))|(anot.locus_tag).isin(hits)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by term to see the biggest / most representative cats\n",
    "(hits_anot.groupby(['term', 'description'])\n",
    " .locus_tag.nunique()\n",
    " .reset_index()\n",
    " .sort_values('locus_tag', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = [('GO:0005976','Lipopolysaccharide/O-Antigen biosynthesis',56), #Polysaccharide metabolic process\n",
    "        ('GO:0008653','Lipopolysaccharide/O-Antigen biosynthesis',33), # Lipopolysaccharide metabolic process\n",
    "        ('GO:1903509','Lipopolysaccharide/O-Antigen biosynthesis',33), # Liposaccharide metabolic process\n",
    "        ('CL:7099',\"Secretion, and Virulence\",35), #\"Mixed, incl. Secretion, and Virulence\"\n",
    "         ('map03070','Secretion, and Virulence',24), #'Bacterial secretion system'\n",
    "         ('GO:0006259','DNA metabolic process',53),\n",
    "         ('GO:0006310','DNA metabolic process',24), #'DNA recombination'\n",
    "         ('CL:546',\"DNA metabolic process\",15), #\"DNA replication, and DNA repair\"\n",
    "         ('GO:0006520','Cellular amino acid metabolic process',79),\n",
    "         ('GO:0006629','Lipid metabolic process',55),\n",
    "        ('GO:0006950','Response to stress/stimulus',63), #'Response to stress' \n",
    "         ('map02024','Response to stress/stimulus',57), #'Quorum sensing'\n",
    "         ('GO:0050896','Response to stress/stimulus',130), #'Response to stimulus'\n",
    "         ('GO:0019222','Regulation of metabolic process',146),\n",
    "         ('map01100','Metabolic process',465), #'Metabolic pathways'\n",
    "         ('GO:0008152','Metabolic process',625),\n",
    "         ('GO:0006810','Transport',243),\n",
    "         ('map02010','Transport',52), #'ABC transporters'\n",
    "         ('CL:4188','Transport',38), #ABC transporters\n",
    "        ('CL:4747','Lipopolysaccharide/O-Antigen biosynthesis',22), #\"Mixed, incl. O-Antigen nucleotide sugar biosynthesis, and Extracellular polysaccharide metabolic process\"\n",
    "         ('map00010','Glycolysis / Gluconeogenesis',24),\n",
    "         ('map00020','Citrate cycle (TCA cycle)',17),\n",
    "         ('GO:0043711','Pilus/Fimbria',10), #'Pilus organization'\n",
    "         ('GO:0015473','Pilus/Fimbria',7), #'Fimbrial usher porin activity'\n",
    "         ('GOCC:0042995','Pilus/Fimbria',18), #'Cell projection'\n",
    "         ('GO:0006457','Protein folding', 23),\n",
    "         ('GO:0044183','Protein folding',10), #'Protein folding chaperone'\n",
    "         ('GO:0042254','Ribosome biogenesis',16),\n",
    "         ('CL:8097',\"Bacteriophage\",39), #Mixed, incl. Viral life cycle, and Coiled coil\n",
    "         ('GO:0009117','Nucleotide metabolic process',39),\n",
    "         ('CL:7533',\"Motility\",41), #\"Mixed, incl. Flagellar assembly, and Bacterial chemotaxis\"\n",
    "         ('GO:0005975','Carbohydrate metabolic process',131),\n",
    "         ('GO:0010467','Gene expression',51),\n",
    "         ('GO:0009058','Biosynthetic process', 218), #Biosynthetic process\n",
    "         ('GO:0009056','Catabolic process',94),\n",
    "         ('GO:0003677','DNA binding',148),\n",
    "         ('GO:0031224','Membrane',346), #Intrinsic component of membrane\n",
    "         ('GO:0016020','Membrane',454),\n",
    "         ('KW-0732','Signal',163),\n",
    "         ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_anot = {}\n",
    "for t in terms: \n",
    "    genes = hits_anot[hits_anot.term == t[0]].locus_tag.values\n",
    "    \n",
    "    for gene in genes:\n",
    "        if gene not in final_anot.keys():\n",
    "            final_anot[gene] = t[1]\n",
    "print(len(final_anot.keys()))\n",
    "\n",
    "for h in hits_anot.locus_tag.unique():\n",
    "    if h not in final_anot.keys():\n",
    "        final_anot[h] = 'Other'\n",
    "print(len(final_anot.keys()))\n",
    "final_anot = pd.DataFrame.from_dict(final_anot, orient='index').reset_index()\n",
    "final_anot.columns = ['locus_tag', 'Function']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sushi_colors = [\"#C0504D\", \"#F79646\", \"#4BACC6\",\n",
    "                \"#00B050\", \"#92D050\", \"#366092\", \"#95B3D7\",\n",
    "                \"#808080\",  \"#D9D9D9\",  \"#FB8072\", \"#BC80BD\",\n",
    "                \"#377EB8\", \"#CCEBC5â€‹\", \"#BEBADA\", \"#CCEBC5\", \"#FCCDE5\",\n",
    "                \"#FFFFB3\", \"#4DAF4A\",]\n",
    "\n",
    "to_keep = ['Metabolic process',\n",
    "           'Lipopolysaccharide/O-Antigen biosynthesis',\n",
    "           'Regulation of metabolic process',\n",
    "           'Response to stress/stimulus',\n",
    "           'Transport',\n",
    "           'DNA metabolic process',\n",
    "           'Cellular amino acid metabolic process',\n",
    "           'Secretion, and Virulence', 'Other',\n",
    "           ]\n",
    "category_colors = {c: col for c, col in zip(\n",
    "    to_keep, sushi_colors[:len(to_keep)])}\n",
    "\n",
    "\n",
    "def format_color_groups(df):\n",
    "    x = df.copy()\n",
    "    for factor in df.Function.unique():\n",
    "        style = f'background-color: {category_colors[factor]}; opacity: 0.5'\n",
    "        x.loc[x['Function'] == factor, :] = style\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_summary_short = final_summary[['Name', 'FC_d1', 'FC_d2', 'FC_d3', 'FC_d4', \n",
    "                                     'padj_range_d1','padj_range_d2', 'padj_range_d3','padj_range_d4']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitness defects\n",
    "s = num_hit.query('number_of_times_detected_as_hit > 1 & hit == -1 & LFC_median < 0')\n",
    "s = (s.drop(['number_of_libraries_with_mutant'], axis=1)\n",
    "          .drop_duplicates()\n",
    "          .rename(columns={'Name':'gene'}))\n",
    "s= s.merge(cds_short, on='gene').merge(final_anot, on='locus_tag')\n",
    "\n",
    "s['cat_ab'] = s.groupby('Function').locus_tag.transform('count')\n",
    "s.loc[s.cat_ab < 15, 'Function'] = 'Other'\n",
    "\n",
    "hi_conf_anotated_hits = s[['gene', 'locus_tag', 'product', 'Function', 'day']].drop_duplicates().copy()\n",
    "hits_from_figure = (hi_conf_anotated_hits[['gene', 'locus_tag','product', 'Function']]\n",
    "                    .merge(final_summary_short, left_on='gene', right_on='Name', how='left').drop_duplicates())\n",
    "# (hits_from_figure.sort_values(['Function', 'locus_tag'])\n",
    "#  .style.apply(format_color_groups, axis=None)\n",
    "#  .to_excel(out_dir/'functional_annotation_of_hits_down_matching_figure1H.xlsx', engine='openpyxl'))\n",
    "\n",
    "s = s.groupby(['day', 'Function']).locus_tag.nunique().reset_index()\n",
    "s = s.sort_values('locus_tag', ascending=False)\n",
    "\n",
    "prefix = 'functional_annotation_of_hits_down'\n",
    "w = 2000\n",
    "h=500\n",
    "fig = px.bar(s, y='Function', x='locus_tag', \n",
    "       color='Function', width=w, height=h, text = 'locus_tag', \n",
    "       template='plotly_white',  orientation='h', facet_col='day', facet_col_spacing=0.02,\n",
    "       #color_discrete_sequence=list(sushi_colors.values()),\n",
    "       color_discrete_map=category_colors,\n",
    "       category_orders={\"day\": [\"d1\", \"d2\", \"d3\", \"d4\"],\n",
    "                        \"Function\": to_keep},\n",
    "                                   labels=({'Function': \"\", \"locus_tag\":\"\"}))\n",
    "fig.update_layout(font=dict(size=20), showlegend=False, font_family=\"Arial\")\n",
    "#pio.write_image(fig, out_dir/f\"{prefix}.svg\", width=w, height=h, scale=2)\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitness advantage\n",
    "s = num_hit.query('number_of_times_detected_as_hit > 1 & hit == 1')\n",
    "s = (s.drop(['number_of_libraries_with_mutant'], axis=1)\n",
    "     .drop_duplicates()\n",
    "     .rename(columns={'Name': 'gene'}))\n",
    "s = s.merge(cds_short, on='gene').merge(final_anot, on='locus_tag')\n",
    "s['cat_ab'] = s.groupby('Function').locus_tag.transform('count')\n",
    "s.loc[s.cat_ab < 6, 'Function'] = 'Other'\n",
    "hi_conf_anotated_hits = s[['gene', 'locus_tag',\n",
    "                           'product', 'day', 'Function']].drop_duplicates().copy()\n",
    "hits_from_figure = (hi_conf_anotated_hits[['gene', 'locus_tag', 'product', 'Function']]\n",
    "                    .merge(final_summary_short, left_on='gene', right_on='Name', how='left').drop_duplicates())\n",
    "# (hits_from_figure.sort_values(['Function', 'locus_tag'])\n",
    "#  .style.apply(format_color_groups, axis=None)\n",
    "#  .to_excel(out_dir/'functional_annotation_of_hits_up_matching_figure1H.xlsx', engine='openpyxl'))\n",
    "\n",
    "s = s.groupby(['day', 'Function']).locus_tag.nunique().reset_index()\n",
    "s = s.sort_values('locus_tag', ascending=False)\n",
    "\n",
    "prefix = 'functional_annotation_of_hits_up'\n",
    "w = 2000\n",
    "h = 500\n",
    "fig = px.bar(s, y='Function', x='locus_tag',\n",
    "             color='Function', width=w, height=h, text='locus_tag',\n",
    "             template='plotly_white',  orientation='h', facet_col='day', facet_col_spacing=0.02,\n",
    "             color_discrete_map=category_colors,\n",
    "             category_orders={\"day\": [\"d1\", \"d2\", \"d3\", \"d4\"],\n",
    "                              \"Function\": ['Metabolic process',\n",
    "                                           'Secretion, and Virulence',\n",
    "                                           'Regulation of metabolic process',\n",
    "                                           'Response to stress/stimulus',\n",
    "                                           'Transport',\n",
    "                                           'DNA metabolic process',\n",
    "                                           'Cellular amino acid metabolic process',\n",
    "                                           'Other',\n",
    "                                           ]},\n",
    "             labels=({'Function': \"\", \"locus_tag\": \"\"}))\n",
    "fig.update_layout(font=dict(size=20), showlegend=False, font_family=\"Arial\")\n",
    "fig.update_xaxes(range=[0, 45])\n",
    "fig.update_yaxes(range=[-0.5, 8])\n",
    "\n",
    "# pio.write_image(fig, out_dir/f\"{prefix}.svg\", width=w, height=h, scale=2)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inoculum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "data_dir = root/config_dict['inoculum_dir']\n",
    "result_files = data_dir.rglob(\"*_rra_results.csv\")\n",
    "fdf = pd.concat([pd.read_csv(f).assign(library=f.stem.split(\"_rra\")[0]) for f in result_files])\n",
    "fdf['padj'] = fdf[['neg_selection_fdr', 'pos_selection_fdr']].min(axis=1)\n",
    "fdf['hit'] = fdf.apply(identify_hits, axis=1)\n",
    "fdf = fdf[fdf.Name.str.len() < 10]\n",
    "rsig = fdf[fdf.hit != 0].copy()\n",
    "num_hits = rsig.groupby(['Name', 'hit']).library.nunique().reset_index().rename(columns={'library': 'num_hit'})\n",
    "# Identify ambigious hits, i.e. hits that were sometimes negative and sometimes positive\n",
    "amb_hits = rsig.groupby(['Name']).hit.value_counts(normalize=True)\n",
    "amb_hits = pd.DataFrame(amb_hits).rename(columns={'hit':'proportion'}).reset_index()\n",
    " \n",
    "# # These were identified as negative (or positive) hits 100% of the time\n",
    "true_hits = amb_hits[amb_hits.proportion == 1]\n",
    "rsig = true_hits.rename(columns={'hit': 'library_specific_hit'}).merge(rsig, on=['Name'],how='left')\n",
    "rsig = rsig.merge(num_hits, on=['Name',  'hit'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate information \n",
    "rsig_agg = rsig[['Name', 'contrast', 'hit', 'num_hit']].drop_duplicates().copy()\n",
    "num_hit = (fdf.groupby(['contrast', 'Name'])\n",
    "           .agg({'library':['nunique'], 'LFC':['median'], 'padj':['min', 'max']})\n",
    "           .reset_index())\n",
    "num_hit.columns = ['contrast', 'Name', 'num_lib', 'LFC_median', 'padj_min', 'padj_max']\n",
    "num_hit =  (num_hit.merge(rsig_agg, on=['contrast', 'Name'], how='outer'))\n",
    "num_hit['num_hit'] = num_hit['num_hit'].fillna(0).astype(int)\n",
    "num_hit['hit'] = num_hit['hit'].fillna(0).astype(int)\n",
    "num_hit['Detection frequency'] = num_hit['num_hit'].astype(str) + \"/\" + num_hit['num_lib'].astype(str)\n",
    "num_hit.columns = ['day', 'Name', 'number_of_libraries_with_mutant', 'LFC_median', 'padj_min', 'padj_max',  'hit','number_of_times_detected_as_hit', 'Detection frequency']\n",
    "num_hit['padj_min_formatted'] = num_hit['padj_min'].apply(lambda x: '< 0.01' if x < 0.01 else str(round(x, 2)))\n",
    "num_hit['padj_max_formatted'] = num_hit['padj_max'].apply(lambda x: '< 0.01' if x < 0.01 else str(round(x, 2)))\n",
    "num_hit['padj_range'] = num_hit['padj_min_formatted'] + \" - \" + num_hit['padj_max_formatted']\n",
    "num_hit['FC'] = 2**num_hit['LFC_median']\n",
    "num_hit['FC_with_detection_frequency'] = num_hit['FC'].round(2).astype(str) + \" (\" + num_hit['Detection frequency'] + \")\"\n",
    "num_hit['FC'] = num_hit[\"FC\"].round(4) \n",
    "num_hit_short = num_hit[['Name', 'day', 'FC_with_detection_frequency', 'padj_range', 'FC', 'LFC_median', 'Detection frequency', 'padj_min_formatted', 'padj_max_formatted', 'padj_min', 'padj_max', 'hit']].sort_values(['day','LFC_median'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#num_hit_short.sort_values(\"Detection frequency\", ascending=False).to_csv(out_dir/\"inoculum-results-gene-level-summary.csv\", index=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datavis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
